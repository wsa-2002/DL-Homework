{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oM8zS-9X6SBf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM8zS-9X6SBf",
    "outputId": "3f50cd68-0018-491c-a918-62be6127e9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JwJQ43Yi7g_4",
   "metadata": {
    "id": "JwJQ43Yi7g_4"
   },
   "source": [
    "# Run this cell to enable kaggle api if you have uploaded your kaggle credential key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9iv_rGgA6fa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iv_rGgA6fa7",
    "outputId": "ca873ff1-fe75-4af8-c10b-1f2773389a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 111ntu-homework3.zip to /content\n",
      "100% 327k/327k [00:00<00:00, 951kB/s]\n",
      "100% 327k/327k [00:00<00:00, 950kB/s]\n",
      "Archive:  111ntu-homework3.zip\n",
      "  inflating: testing_x.npy           \n",
      "  inflating: training_x.npy          \n",
      "  inflating: training_y.npy          \n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp ./drive/Shareddrives/ml/深度學習/kaggle.json ~/.kaggle/\n",
    "!! chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c 111ntu-homework3\n",
    "!unzip 111ntu-homework3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3213023",
   "metadata": {
    "id": "a3213023",
    "papermill": {
     "duration": 0.0046,
     "end_time": "2022-09-19T05:17:25.003537",
     "exception": false,
     "start_time": "2022-09-19T05:17:24.998937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GP1sWvtKHi-P",
   "metadata": {
    "id": "GP1sWvtKHi-P"
   },
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# for saving record\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# For plotting learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L9rWvlTpAbmb",
   "metadata": {
    "id": "L9rWvlTpAbmb"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "AeqT_CNcAd9y",
   "metadata": {
    "id": "AeqT_CNcAd9y"
   },
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    same_seed(config['seed'])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=config['weight_decay'], amsgrad=False)\n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_record = []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()              \n",
    "            x, y = x.to(device), y.to(device)  \n",
    "            pred = model(x, y, config['teacher_forcing_ratio'])             \n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                  \n",
    "            optimizer.step()              \n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "\n",
    "        model.eval()\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x, y, 0)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) \n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n",
    "\n",
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set)) \n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device, config):\n",
    "    model.eval() \n",
    "    preds = []\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)                        \n",
    "        with torch.no_grad():                \n",
    "            pred = model(x, y, 0)          \n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds\n",
    "\n",
    "def select_feat(train_data, valid_data, test_data, select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n",
    "\n",
    "    if select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        feat_idx =[1, 2, 5, 6, 7, 9] \n",
    "        \n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xGsjMCsnHe7u",
   "metadata": {
    "id": "xGsjMCsnHe7u"
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dr_vnQBd71PF",
   "metadata": {
    "id": "dr_vnQBd71PF"
   },
   "outputs": [],
   "source": [
    "x_training, y_training, x_testing = np.load('training_x.npy'), np.load('training_y.npy'), np.load('testing_x.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c40f04",
   "metadata": {
    "id": "b9c40f04",
    "papermill": {
     "duration": 0.003104,
     "end_time": "2022-09-19T05:17:25.017919",
     "exception": false,
     "start_time": "2022-09-19T05:17:25.014815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIZ0ei5Zj_fp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIZ0ei5Zj_fp",
    "outputId": "6f563eae-7018-450d-d274-70709786f68e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8760, 13), (8760, 1), (8760, 13))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.shape, y_training.shape, x_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uDVlSoPLiAZb",
   "metadata": {
    "id": "uDVlSoPLiAZb"
   },
   "outputs": [],
   "source": [
    "timestep = 8\n",
    "x_training = np.array([x_training[i:i+timestep] for i in range(x_training.shape[0] - timestep * 2)])\n",
    "x_testing = np.array([x_testing[i:i+timestep] for i in range(x_testing.shape[0] - timestep * 2)])\n",
    "y_training = np.array([y_training[i+timestep:i+(2 * timestep)] for i in range(y_training.shape[0] - timestep * 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pq73diN5j4Uq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pq73diN5j4Uq",
    "outputId": "08a4bc80-6cbf-4090-c047-4d56c907ba20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8744, 8, 13), (8744, 8, 1), (8744, 8, 13))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.shape, y_training.shape, x_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77a614",
   "metadata": {
    "id": "9e77a614",
    "papermill": {
     "duration": 0.003072,
     "end_time": "2022-09-19T05:17:25.030853",
     "exception": false,
     "start_time": "2022-09-19T05:17:25.027781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MxXOV3XVqhbk",
   "metadata": {
    "id": "MxXOV3XVqhbk"
   },
   "source": [
    "#### 使用 torch.nn + seq2seq + attention\n",
    "mse loss: 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dSyjJGxVJ4h3",
   "metadata": {
    "id": "dSyjJGxVJ4h3"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size = 13,\n",
    "                 hidden_size = 64,\n",
    "                 num_layers = 4,\n",
    "                 dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                           dropout=dropout, batch_first = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.rnn(x)\n",
    "        return output, hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size=hidden_size+1, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Attention\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, decoder_hidden, encoder_output):\n",
    "        \"\"\"\n",
    "        torch.Size([2, 256, 64]) torch.Size([256, 8, 64])\n",
    "        torch.Size([256, 2, 64])\n",
    "        torch.Size([256, 1, 64])\n",
    "        torch.Size([256, 8, 1])\n",
    "        torch.Size([256, 8, 1])\n",
    "        torch.Size([256, 64])\n",
    "        torch.Size([256, 65])\n",
    "        torch.Size([256, 1, 65])\n",
    "        torch.Size([256, 1, 64]) torch.Size([2, 1, 64])\n",
    "        torch.Size([256, 64])\n",
    "        torch.Size([256, 1])\n",
    "        \"\"\"\n",
    "        # Attention\n",
    "        # (2, 256, 64, 256, 8, 64)\n",
    "        decoder_hidden = decoder_hidden.view(decoder_hidden.shape[1], decoder_hidden.shape[0], self.hidden_size) # (256, 2, 64)\n",
    "        decoder_hidden = decoder_hidden[:, -1, :].unsqueeze(1) # (256, 1, 64)\n",
    "        energy = self.v(self.tanh(self.W1(encoder_output) + self.W2(decoder_hidden))) # (256, 8, 1)\n",
    "        attention_weight = self.softmax(energy) # (256, 8, 1)\n",
    "        context_vector = torch.sum(attention_weight * encoder_output, dim=1) # (256, 64)\n",
    "\n",
    "        concat_input = torch.concat([x, context_vector], dim=-1) # (256, 65)\n",
    "        concat_input_fit = concat_input.unsqueeze(1) # (256, 1, 65)\n",
    "        output, hidden = self.rnn(concat_input_fit) # (256, 1, 64), (2, 1, 64)\n",
    "\n",
    "        output = output.squeeze(1) # (256, 64)\n",
    "\n",
    "        prediction = self.fc(output) # (256, 1)\n",
    "\n",
    "        return prediction, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        teacher_forcing_ratio: probability of using groud truth instead of training output\n",
    "        set teacher_forcing_ratio to 0 for testing purpose\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        target_len = y.shape[1]\n",
    "        outputs = torch.zeros(y.shape).to(self.device)\n",
    "        encoder_output, hidden = self.encoder(x)\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.float).to(self.device)\n",
    "        for i in range(target_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden, encoder_output)\n",
    "            outputs[:,i] = output\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = y[:,i] if teacher_forcing else output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadcf91",
   "metadata": {
    "id": "6dadcf91",
    "papermill": {
     "duration": 0.003124,
     "end_time": "2022-09-19T05:17:25.044350",
     "exception": false,
     "start_time": "2022-09-19T05:17:25.041226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "q7Xpg5a70_nr",
   "metadata": {
    "id": "q7Xpg5a70_nr"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qGBgfK9Y2o4h",
   "metadata": {
    "id": "qGBgfK9Y2o4h"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 175,      \n",
    "    'select_all': True,   \n",
    "    'valid_ratio': 0.2,  \n",
    "    'n_epochs': 120,        \n",
    "    'batch_size': 256, \n",
    "    'learning_rate': 1e-3,          \n",
    "    'early_stop': 400,    \n",
    "    'save_path': './models/b09705017_王紹安.ckpt',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-6,\n",
    "    'input_size': x_training.shape[2],\n",
    "    'hidden_size': 64,\n",
    "    'output_size': 1,\n",
    "    'teacher_forcing_ratio': 0.5,\n",
    "    'num_layers': 2,\n",
    "    'encoder': {\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'decoder': {\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jn5xMyFD1FBm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn5xMyFD1FBm",
    "outputId": "8de2d048-16a2-4936-b3a2-5875edb7e52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: (6996, 8, 13)\n",
      "valid_data size: (1748, 8, 13)\n",
      "test_data size: (8744, 8, 13)\n",
      "number of features: 13\n"
     ]
    }
   ],
   "source": [
    "same_seed(config['seed'])\n",
    "\n",
    "x_train, x_valid = train_valid_split(x_training, config['valid_ratio'], config['seed'])\n",
    "y_train, y_valid = train_valid_split(y_training, config['valid_ratio'], config['seed'])\n",
    "x_test = x_testing\n",
    "# Print out the data size.\n",
    "print(f\"\"\"train_data size: {x_train.shape}\n",
    "valid_data size: {x_valid.shape}\n",
    "test_data size: {x_test.shape}\"\"\")\n",
    "\n",
    "# Select features\n",
    "# x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])\n",
    "\n",
    "# Print out the number of features.\n",
    "print(f'number of features: {x_train.shape[2]}')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = CustomDataset(x_train, y_train), \\\n",
    "                                            CustomDataset(x_valid, y_valid), \\\n",
    "                                            CustomDataset(x_test, y_training) # y_training is dummy tensor, only needs the shape of it.\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "C2Vpx4Keereq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2Vpx4Keereq",
    "outputId": "e89093f4-48a6-49e3-8a7f-706b97776397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120]: Train loss: 768.1933, Valid loss: 699.3330\n",
      "Saving model with loss 699.333...\n",
      "Epoch [2/120]: Train loss: 618.7797, Valid loss: 605.3973\n",
      "Saving model with loss 605.397...\n",
      "Epoch [3/120]: Train loss: 541.4111, Valid loss: 536.0429\n",
      "Saving model with loss 536.043...\n",
      "Epoch [4/120]: Train loss: 482.9065, Valid loss: 478.9467\n",
      "Saving model with loss 478.947...\n",
      "Epoch [5/120]: Train loss: 430.7972, Valid loss: 430.8225\n",
      "Saving model with loss 430.823...\n",
      "Epoch [6/120]: Train loss: 389.0450, Valid loss: 390.7300\n",
      "Saving model with loss 390.730...\n",
      "Epoch [7/120]: Train loss: 352.1788, Valid loss: 358.3255\n",
      "Saving model with loss 358.326...\n",
      "Epoch [8/120]: Train loss: 323.4513, Valid loss: 329.6020\n",
      "Saving model with loss 329.602...\n",
      "Epoch [9/120]: Train loss: 300.3755, Valid loss: 306.7133\n",
      "Saving model with loss 306.713...\n",
      "Epoch [10/120]: Train loss: 284.4927, Valid loss: 289.8104\n",
      "Saving model with loss 289.810...\n",
      "Epoch [11/120]: Train loss: 271.1350, Valid loss: 277.3538\n",
      "Saving model with loss 277.354...\n",
      "Epoch [12/120]: Train loss: 256.3121, Valid loss: 265.6919\n",
      "Saving model with loss 265.692...\n",
      "Epoch [13/120]: Train loss: 249.0355, Valid loss: 255.6775\n",
      "Saving model with loss 255.677...\n",
      "Epoch [14/120]: Train loss: 240.7747, Valid loss: 249.0895\n",
      "Saving model with loss 249.089...\n",
      "Epoch [15/120]: Train loss: 236.3689, Valid loss: 244.5172\n",
      "Saving model with loss 244.517...\n",
      "Epoch [16/120]: Train loss: 232.7030, Valid loss: 239.3616\n",
      "Saving model with loss 239.362...\n",
      "Epoch [17/120]: Train loss: 231.4822, Valid loss: 237.6056\n",
      "Saving model with loss 237.606...\n",
      "Epoch [18/120]: Train loss: 227.9145, Valid loss: 234.4947\n",
      "Saving model with loss 234.495...\n",
      "Epoch [19/120]: Train loss: 228.1605, Valid loss: 233.9597\n",
      "Saving model with loss 233.960...\n",
      "Epoch [20/120]: Train loss: 224.0832, Valid loss: 230.8356\n",
      "Saving model with loss 230.836...\n",
      "Epoch [21/120]: Train loss: 222.6921, Valid loss: 232.2221\n",
      "Epoch [22/120]: Train loss: 224.4096, Valid loss: 232.3987\n",
      "Epoch [23/120]: Train loss: 224.7051, Valid loss: 229.4109\n",
      "Saving model with loss 229.411...\n",
      "Epoch [24/120]: Train loss: 224.9410, Valid loss: 229.0789\n",
      "Saving model with loss 229.079...\n",
      "Epoch [25/120]: Train loss: 224.4752, Valid loss: 229.6972\n",
      "Epoch [26/120]: Train loss: 222.7242, Valid loss: 228.5374\n",
      "Saving model with loss 228.537...\n",
      "Epoch [27/120]: Train loss: 222.0983, Valid loss: 228.3156\n",
      "Saving model with loss 228.316...\n",
      "Epoch [28/120]: Train loss: 224.2117, Valid loss: 228.9649\n",
      "Epoch [29/120]: Train loss: 223.2000, Valid loss: 231.2022\n",
      "Epoch [30/120]: Train loss: 221.9746, Valid loss: 229.7227\n",
      "Epoch [31/120]: Train loss: 224.3480, Valid loss: 229.1480\n",
      "Epoch [32/120]: Train loss: 222.9870, Valid loss: 227.7911\n",
      "Saving model with loss 227.791...\n",
      "Epoch [33/120]: Train loss: 222.4840, Valid loss: 229.5273\n",
      "Epoch [34/120]: Train loss: 222.7673, Valid loss: 228.6881\n",
      "Epoch [35/120]: Train loss: 223.5855, Valid loss: 229.4739\n",
      "Epoch [36/120]: Train loss: 226.8522, Valid loss: 228.4144\n",
      "Epoch [37/120]: Train loss: 222.3756, Valid loss: 229.4716\n",
      "Epoch [38/120]: Train loss: 223.4676, Valid loss: 228.4446\n",
      "Epoch [39/120]: Train loss: 221.8083, Valid loss: 229.4583\n",
      "Epoch [40/120]: Train loss: 222.7772, Valid loss: 228.5615\n",
      "Epoch [41/120]: Train loss: 223.3914, Valid loss: 229.9873\n",
      "Epoch [42/120]: Train loss: 222.4144, Valid loss: 228.9207\n",
      "Epoch [43/120]: Train loss: 222.6756, Valid loss: 228.0508\n",
      "Epoch [44/120]: Train loss: 221.4071, Valid loss: 228.9199\n",
      "Epoch [45/120]: Train loss: 221.3486, Valid loss: 227.8908\n",
      "Epoch [46/120]: Train loss: 222.2624, Valid loss: 228.6952\n",
      "Epoch [47/120]: Train loss: 221.9310, Valid loss: 228.9023\n",
      "Epoch [48/120]: Train loss: 221.8902, Valid loss: 230.2169\n",
      "Epoch [49/120]: Train loss: 222.8995, Valid loss: 228.7483\n",
      "Epoch [50/120]: Train loss: 225.1179, Valid loss: 228.3682\n",
      "Epoch [51/120]: Train loss: 222.3100, Valid loss: 228.4264\n",
      "Epoch [52/120]: Train loss: 222.2966, Valid loss: 228.0876\n",
      "Epoch [53/120]: Train loss: 223.8982, Valid loss: 227.9723\n",
      "Epoch [54/120]: Train loss: 222.2161, Valid loss: 228.6915\n",
      "Epoch [55/120]: Train loss: 221.4606, Valid loss: 228.1528\n",
      "Epoch [56/120]: Train loss: 223.2808, Valid loss: 229.4061\n",
      "Epoch [57/120]: Train loss: 223.1748, Valid loss: 228.5780\n",
      "Epoch [58/120]: Train loss: 222.3030, Valid loss: 228.0244\n",
      "Epoch [59/120]: Train loss: 224.2430, Valid loss: 227.9293\n",
      "Epoch [60/120]: Train loss: 220.9646, Valid loss: 225.8514\n",
      "Saving model with loss 225.851...\n",
      "Epoch [61/120]: Train loss: 195.2025, Valid loss: 196.2498\n",
      "Saving model with loss 196.250...\n",
      "Epoch [62/120]: Train loss: 166.0043, Valid loss: 210.9099\n",
      "Epoch [63/120]: Train loss: 154.7146, Valid loss: 183.2689\n",
      "Saving model with loss 183.269...\n",
      "Epoch [64/120]: Train loss: 137.5673, Valid loss: 165.6010\n",
      "Saving model with loss 165.601...\n",
      "Epoch [65/120]: Train loss: 130.1133, Valid loss: 166.8927\n",
      "Epoch [66/120]: Train loss: 125.6283, Valid loss: 169.7649\n",
      "Epoch [67/120]: Train loss: 116.9524, Valid loss: 151.6114\n",
      "Saving model with loss 151.611...\n",
      "Epoch [68/120]: Train loss: 112.4057, Valid loss: 150.3713\n",
      "Saving model with loss 150.371...\n",
      "Epoch [69/120]: Train loss: 109.8895, Valid loss: 147.2013\n",
      "Saving model with loss 147.201...\n",
      "Epoch [70/120]: Train loss: 104.0469, Valid loss: 149.8174\n",
      "Epoch [71/120]: Train loss: 99.6159, Valid loss: 140.4466\n",
      "Saving model with loss 140.447...\n",
      "Epoch [72/120]: Train loss: 97.7983, Valid loss: 134.7271\n",
      "Saving model with loss 134.727...\n",
      "Epoch [73/120]: Train loss: 91.4110, Valid loss: 136.0743\n",
      "Epoch [74/120]: Train loss: 93.4387, Valid loss: 132.0820\n",
      "Saving model with loss 132.082...\n",
      "Epoch [75/120]: Train loss: 90.2674, Valid loss: 130.7381\n",
      "Saving model with loss 130.738...\n",
      "Epoch [76/120]: Train loss: 87.6182, Valid loss: 131.6450\n",
      "Epoch [77/120]: Train loss: 87.4650, Valid loss: 126.4259\n",
      "Saving model with loss 126.426...\n",
      "Epoch [78/120]: Train loss: 84.0105, Valid loss: 127.5485\n",
      "Epoch [79/120]: Train loss: 83.5771, Valid loss: 126.5708\n",
      "Epoch [80/120]: Train loss: 81.5785, Valid loss: 124.0870\n",
      "Saving model with loss 124.087...\n",
      "Epoch [81/120]: Train loss: 81.6249, Valid loss: 127.2091\n",
      "Epoch [82/120]: Train loss: 78.9503, Valid loss: 138.3101\n",
      "Epoch [83/120]: Train loss: 83.4182, Valid loss: 129.9965\n",
      "Epoch [84/120]: Train loss: 82.1529, Valid loss: 126.8653\n",
      "Epoch [85/120]: Train loss: 79.9001, Valid loss: 123.2668\n",
      "Saving model with loss 123.267...\n",
      "Epoch [86/120]: Train loss: 78.0917, Valid loss: 127.0607\n",
      "Epoch [87/120]: Train loss: 73.1518, Valid loss: 125.3939\n",
      "Epoch [88/120]: Train loss: 75.8080, Valid loss: 120.3968\n",
      "Saving model with loss 120.397...\n",
      "Epoch [89/120]: Train loss: 75.8530, Valid loss: 119.8747\n",
      "Saving model with loss 119.875...\n",
      "Epoch [90/120]: Train loss: 74.4264, Valid loss: 120.8438\n",
      "Epoch [91/120]: Train loss: 71.2010, Valid loss: 117.5763\n",
      "Saving model with loss 117.576...\n",
      "Epoch [92/120]: Train loss: 72.5388, Valid loss: 118.2686\n",
      "Epoch [93/120]: Train loss: 74.4701, Valid loss: 115.5128\n",
      "Saving model with loss 115.513...\n",
      "Epoch [94/120]: Train loss: 70.3142, Valid loss: 116.2299\n",
      "Epoch [95/120]: Train loss: 75.3651, Valid loss: 114.7429\n",
      "Saving model with loss 114.743...\n",
      "Epoch [96/120]: Train loss: 71.2371, Valid loss: 115.8821\n",
      "Epoch [97/120]: Train loss: 69.0331, Valid loss: 116.8520\n",
      "Epoch [98/120]: Train loss: 71.2330, Valid loss: 111.9429\n",
      "Saving model with loss 111.943...\n",
      "Epoch [99/120]: Train loss: 69.8262, Valid loss: 123.0128\n",
      "Epoch [100/120]: Train loss: 65.6996, Valid loss: 112.6141\n",
      "Epoch [101/120]: Train loss: 69.6677, Valid loss: 110.2992\n",
      "Saving model with loss 110.299...\n",
      "Epoch [102/120]: Train loss: 68.7522, Valid loss: 119.6347\n",
      "Epoch [103/120]: Train loss: 65.5713, Valid loss: 112.0511\n",
      "Epoch [104/120]: Train loss: 62.7412, Valid loss: 108.5940\n",
      "Saving model with loss 108.594...\n",
      "Epoch [105/120]: Train loss: 63.9867, Valid loss: 109.7439\n",
      "Epoch [106/120]: Train loss: 65.9069, Valid loss: 107.5551\n",
      "Saving model with loss 107.555...\n",
      "Epoch [107/120]: Train loss: 64.3048, Valid loss: 110.0536\n",
      "Epoch [108/120]: Train loss: 64.1062, Valid loss: 111.3667\n",
      "Epoch [109/120]: Train loss: 67.5505, Valid loss: 107.7045\n",
      "Epoch [110/120]: Train loss: 65.9045, Valid loss: 111.8719\n",
      "Epoch [111/120]: Train loss: 62.5675, Valid loss: 109.0813\n",
      "Epoch [112/120]: Train loss: 60.2659, Valid loss: 109.4646\n",
      "Epoch [113/120]: Train loss: 63.0126, Valid loss: 109.7111\n",
      "Epoch [114/120]: Train loss: 58.8144, Valid loss: 106.8713\n",
      "Saving model with loss 106.871...\n",
      "Epoch [115/120]: Train loss: 62.5905, Valid loss: 110.2303\n",
      "Epoch [116/120]: Train loss: 56.6931, Valid loss: 108.4171\n",
      "Epoch [117/120]: Train loss: 62.7881, Valid loss: 111.1841\n",
      "Epoch [118/120]: Train loss: 67.9520, Valid loss: 140.5947\n",
      "Epoch [119/120]: Train loss: 76.4170, Valid loss: 122.3392\n",
      "Epoch [120/120]: Train loss: 71.4472, Valid loss: 119.3878\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size=config['input_size'], hidden_size=config['hidden_size'], num_layers=config['num_layers'], dropout=config['encoder']['dropout'])\n",
    "decoder = Decoder(config['hidden_size'], num_layers=config['num_layers'], dropout=config['decoder']['dropout'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efa4d9",
   "metadata": {
    "id": "65efa4d9",
    "papermill": {
     "duration": 0.003053,
     "end_time": "2022-09-19T05:17:25.057417",
     "exception": false,
     "start_time": "2022-09-19T05:17:25.054364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Re-load best weight, and predict test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Tl0_GLD61YUr",
   "metadata": {
    "id": "Tl0_GLD61YUr"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size=config['input_size'], hidden_size=config['hidden_size'], num_layers=config['num_layers'], dropout=config['encoder']['dropout'])\n",
    "decoder = Decoder(config['hidden_size'], num_layers=config['num_layers'], dropout=config['decoder']['dropout'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48724c82",
   "metadata": {
    "id": "48724c82",
    "papermill": {
     "duration": 0.00314,
     "end_time": "2022-09-19T05:17:25.070364",
     "exception": false,
     "start_time": "2022-09-19T05:17:25.067224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# save test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hByMaLe-1f7k",
   "metadata": {
    "id": "hByMaLe-1f7k"
   },
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['INDEX', *[f'PM2.5-{i+1}' for i in range(8)]])\n",
    "        for i, pred in enumerate(preds):\n",
    "            # if p2 > p1:\n",
    "            #     p1, p2 = p2, p1\n",
    "            writer.writerow([i+1, *pred])\n",
    "save_pred(preds.squeeze(), 'b09705017_王紹安.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ndj6IP4oLa2L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndj6IP4oLa2L",
    "outputId": "abb59ae8-9243-41ba-ff20-39c53ed3f2aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 715k/715k [00:01<00:00, 678kB/s]\n",
      "Successfully submitted to 111NTU Homework3"
     ]
    }
   ],
   "source": [
    "submit_time = (datetime.now() + timedelta(hours=8)).strftime('%m%d%H%M')\n",
    "!kaggle competitions submit -c 111ntu-homework3 -f b09705017_王紹安.csv -m {submit_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vmi_uJEDdDVd",
   "metadata": {
    "id": "vmi_uJEDdDVd"
   },
   "outputs": [],
   "source": [
    "score = 104\n",
    "if not os.path.isdir(f'./drive/Shareddrives/ml/深度學習/hw3/{submit_time}_{score}'):\n",
    "    os.mkdir(f'./drive/Shareddrives/ml/深度學習/hw3/{submit_time}_{score}')\n",
    "name = str(submit_time + '_' + str(score))\n",
    "%notebook -e ./drive/Shareddrives/ml/深度學習/hw3/$name/b09705017_王紹安.ipynb\n",
    "save_pred(preds, f'./drive/Shareddrives/ml/深度學習/hw3/{submit_time}_{score}/b09705017_王紹安.csv')\n",
    "%cp ./models/b09705017_王紹安.ckpt ./drive/Shareddrives/ml/深度學習/hw3/$name/b09705017_王紹安.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l_2ldZ5bx4C8",
   "metadata": {
    "id": "l_2ldZ5bx4C8"
   },
   "source": [
    "### 手刻 seq2seq\n",
    "mse: 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fexfJS8qQFo",
   "metadata": {
    "id": "4fexfJS8qQFo"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 175,      \n",
    "    'select_all': True,   \n",
    "    'valid_ratio': 0.2,  \n",
    "    'n_epochs': 800,        \n",
    "    'batch_size': 512, \n",
    "    'learning_rate': 1e-5,          \n",
    "    'early_stop': 400,    \n",
    "    'save_path': './models/b09705017_王紹安.ckpt',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-5,\n",
    "    'input_size': x_training.shape[2],\n",
    "    'hidden_size': 64,\n",
    "    'output_size': 1,\n",
    "    'teacher_forcing_ratio': 0.5,\n",
    "    'num_layers': 2,\n",
    "    'encoder': {\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'decoder': {\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YGY21XD0YNIv",
   "metadata": {
    "id": "YGY21XD0YNIv"
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in).to('cuda')\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        bias = torch.Tensor(size_out).to('cuda')\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x= torch.mm(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size = 13,\n",
    "                 hidden_size = 64,\n",
    "                 num_layers = 4,\n",
    "                 dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.linear1 = MyLinear(input_size, 512)\n",
    "        self.linear2 = MyLinear(512, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.zeros((8, x.shape[0], self.hidden_size)).to('cuda')\n",
    "        for i, data in enumerate(x.permute(1, 0, 2)):\n",
    "            output = self.linear1(data)\n",
    "            output = self.dropout1(output)\n",
    "            output = self.relu1(output)\n",
    "\n",
    "            output = self.linear2(output)\n",
    "            output = self.dropout2(output)\n",
    "            output = self.relu2(output)\n",
    "\n",
    "            outputs[i, :, :] = output\n",
    "        return outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = MyLinear(hidden_size, int(hidden_size / 2))\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.linear2 = MyLinear(int(hidden_size / 2), output_size)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing):\n",
    "        outputs = torch.zeros((8, x.shape[1], 1)).to('cuda')\n",
    "        for i, data in enumerate(x):\n",
    "            output = self.linear(data)\n",
    "            output = self.dropout1(output)\n",
    "            output = self.relu1(output)\n",
    "            output = self.linear2(output)\n",
    "            output = self.dropout2(output)\n",
    "            output = self.relu2(output)\n",
    "            outputs[i, :, :] = output\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing=0):\n",
    "        target_len = y.shape[1]\n",
    "        encoder_output = self.encoder(x)\n",
    "        decoder_output = self.decoder(encoder_output, y, teacher_forcing)\n",
    "        decoder_output = decoder_output.permute(1, 0, 2)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uvWrggD2a_Cp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvWrggD2a_Cp",
    "outputId": "7da498e0-ee8d-40f7-ddb7-05ccb4b8bfdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/800]: Train loss: 872.9366, Valid loss: 919.1982\n",
      "Saving model with loss 919.198...\n",
      "Epoch [2/800]: Train loss: 862.9677, Valid loss: 916.3078\n",
      "Saving model with loss 916.308...\n",
      "Epoch [3/800]: Train loss: 851.2251, Valid loss: 884.7419\n",
      "Saving model with loss 884.742...\n",
      "Epoch [4/800]: Train loss: 833.2215, Valid loss: 855.3817\n",
      "Saving model with loss 855.382...\n",
      "Epoch [5/800]: Train loss: 815.6066, Valid loss: 844.1859\n",
      "Saving model with loss 844.186...\n",
      "Epoch [6/800]: Train loss: 793.5525, Valid loss: 829.0648\n",
      "Saving model with loss 829.065...\n",
      "Epoch [7/800]: Train loss: 769.3176, Valid loss: 776.6666\n",
      "Saving model with loss 776.667...\n",
      "Epoch [8/800]: Train loss: 749.1922, Valid loss: 752.3290\n",
      "Saving model with loss 752.329...\n",
      "Epoch [9/800]: Train loss: 729.0294, Valid loss: 724.1123\n",
      "Saving model with loss 724.112...\n",
      "Epoch [10/800]: Train loss: 704.2593, Valid loss: 705.8696\n",
      "Saving model with loss 705.870...\n",
      "Epoch [11/800]: Train loss: 679.7728, Valid loss: 679.9767\n",
      "Saving model with loss 679.977...\n",
      "Epoch [12/800]: Train loss: 661.5721, Valid loss: 657.8845\n",
      "Saving model with loss 657.885...\n",
      "Epoch [13/800]: Train loss: 640.4080, Valid loss: 635.4797\n",
      "Saving model with loss 635.480...\n",
      "Epoch [14/800]: Train loss: 617.6035, Valid loss: 602.9761\n",
      "Saving model with loss 602.976...\n",
      "Epoch [15/800]: Train loss: 602.3093, Valid loss: 576.1762\n",
      "Saving model with loss 576.176...\n",
      "Epoch [16/800]: Train loss: 578.0243, Valid loss: 559.3516\n",
      "Saving model with loss 559.352...\n",
      "Epoch [17/800]: Train loss: 565.5106, Valid loss: 524.1423\n",
      "Saving model with loss 524.142...\n",
      "Epoch [18/800]: Train loss: 544.9053, Valid loss: 520.1308\n",
      "Saving model with loss 520.131...\n",
      "Epoch [19/800]: Train loss: 532.0261, Valid loss: 506.1050\n",
      "Saving model with loss 506.105...\n",
      "Epoch [20/800]: Train loss: 514.8402, Valid loss: 483.4592\n",
      "Saving model with loss 483.459...\n",
      "Epoch [21/800]: Train loss: 501.7394, Valid loss: 461.5044\n",
      "Saving model with loss 461.504...\n",
      "Epoch [22/800]: Train loss: 490.6199, Valid loss: 445.2861\n",
      "Saving model with loss 445.286...\n",
      "Epoch [23/800]: Train loss: 480.8948, Valid loss: 436.1282\n",
      "Saving model with loss 436.128...\n",
      "Epoch [24/800]: Train loss: 471.9104, Valid loss: 422.1169\n",
      "Saving model with loss 422.117...\n",
      "Epoch [25/800]: Train loss: 462.8537, Valid loss: 408.8869\n",
      "Saving model with loss 408.887...\n",
      "Epoch [26/800]: Train loss: 453.9595, Valid loss: 388.3417\n",
      "Saving model with loss 388.342...\n",
      "Epoch [27/800]: Train loss: 446.5664, Valid loss: 372.0991\n",
      "Saving model with loss 372.099...\n",
      "Epoch [28/800]: Train loss: 439.9289, Valid loss: 359.4378\n",
      "Saving model with loss 359.438...\n",
      "Epoch [29/800]: Train loss: 433.4153, Valid loss: 354.1971\n",
      "Saving model with loss 354.197...\n",
      "Epoch [30/800]: Train loss: 427.9300, Valid loss: 346.7038\n",
      "Saving model with loss 346.704...\n",
      "Epoch [31/800]: Train loss: 423.8334, Valid loss: 336.2123\n",
      "Saving model with loss 336.212...\n",
      "Epoch [32/800]: Train loss: 418.5671, Valid loss: 333.0195\n",
      "Saving model with loss 333.020...\n",
      "Epoch [33/800]: Train loss: 414.8124, Valid loss: 312.0144\n",
      "Saving model with loss 312.014...\n",
      "Epoch [34/800]: Train loss: 408.8651, Valid loss: 311.0897\n",
      "Saving model with loss 311.090...\n",
      "Epoch [35/800]: Train loss: 404.8851, Valid loss: 314.0292\n",
      "Epoch [36/800]: Train loss: 404.1721, Valid loss: 295.4269\n",
      "Saving model with loss 295.427...\n",
      "Epoch [37/800]: Train loss: 405.8338, Valid loss: 307.5290\n",
      "Epoch [38/800]: Train loss: 399.1316, Valid loss: 293.4065\n",
      "Saving model with loss 293.407...\n",
      "Epoch [39/800]: Train loss: 396.7537, Valid loss: 290.7174\n",
      "Saving model with loss 290.717...\n",
      "Epoch [40/800]: Train loss: 401.6455, Valid loss: 284.4347\n",
      "Saving model with loss 284.435...\n",
      "Epoch [41/800]: Train loss: 396.9641, Valid loss: 265.9837\n",
      "Saving model with loss 265.984...\n",
      "Epoch [42/800]: Train loss: 396.8504, Valid loss: 272.3612\n",
      "Epoch [43/800]: Train loss: 397.7636, Valid loss: 274.1508\n",
      "Epoch [44/800]: Train loss: 395.1182, Valid loss: 284.2008\n",
      "Epoch [45/800]: Train loss: 395.7178, Valid loss: 267.6277\n",
      "Epoch [46/800]: Train loss: 395.9861, Valid loss: 268.7783\n",
      "Epoch [47/800]: Train loss: 395.1013, Valid loss: 259.5857\n",
      "Saving model with loss 259.586...\n",
      "Epoch [48/800]: Train loss: 389.8086, Valid loss: 259.7381\n",
      "Epoch [49/800]: Train loss: 393.4321, Valid loss: 258.2281\n",
      "Saving model with loss 258.228...\n",
      "Epoch [50/800]: Train loss: 390.5566, Valid loss: 256.6146\n",
      "Saving model with loss 256.615...\n",
      "Epoch [51/800]: Train loss: 393.5132, Valid loss: 255.5462\n",
      "Saving model with loss 255.546...\n",
      "Epoch [52/800]: Train loss: 391.5501, Valid loss: 256.6297\n",
      "Epoch [53/800]: Train loss: 387.6095, Valid loss: 250.4522\n",
      "Saving model with loss 250.452...\n",
      "Epoch [54/800]: Train loss: 388.5917, Valid loss: 250.5651\n",
      "Epoch [55/800]: Train loss: 386.1725, Valid loss: 259.4091\n",
      "Epoch [56/800]: Train loss: 385.1310, Valid loss: 258.0808\n",
      "Epoch [57/800]: Train loss: 387.5481, Valid loss: 257.2889\n",
      "Epoch [58/800]: Train loss: 387.7838, Valid loss: 255.6820\n",
      "Epoch [59/800]: Train loss: 387.1138, Valid loss: 249.4216\n",
      "Saving model with loss 249.422...\n",
      "Epoch [60/800]: Train loss: 386.7670, Valid loss: 258.8610\n",
      "Epoch [61/800]: Train loss: 383.9408, Valid loss: 252.3818\n",
      "Epoch [62/800]: Train loss: 389.5387, Valid loss: 249.3515\n",
      "Saving model with loss 249.351...\n",
      "Epoch [63/800]: Train loss: 387.8555, Valid loss: 259.1774\n",
      "Epoch [64/800]: Train loss: 381.5758, Valid loss: 246.3648\n",
      "Saving model with loss 246.365...\n",
      "Epoch [65/800]: Train loss: 384.5088, Valid loss: 243.5085\n",
      "Saving model with loss 243.508...\n",
      "Epoch [66/800]: Train loss: 382.2512, Valid loss: 242.6408\n",
      "Saving model with loss 242.641...\n",
      "Epoch [67/800]: Train loss: 382.3808, Valid loss: 237.1150\n",
      "Saving model with loss 237.115...\n",
      "Epoch [68/800]: Train loss: 381.5725, Valid loss: 253.3019\n",
      "Epoch [69/800]: Train loss: 383.0674, Valid loss: 245.7875\n",
      "Epoch [70/800]: Train loss: 385.2009, Valid loss: 249.0659\n",
      "Epoch [71/800]: Train loss: 383.5059, Valid loss: 239.9078\n",
      "Epoch [72/800]: Train loss: 383.7311, Valid loss: 245.1839\n",
      "Epoch [73/800]: Train loss: 380.0861, Valid loss: 240.6696\n",
      "Epoch [74/800]: Train loss: 384.0900, Valid loss: 254.3756\n",
      "Epoch [75/800]: Train loss: 377.3702, Valid loss: 240.0054\n",
      "Epoch [76/800]: Train loss: 379.6361, Valid loss: 244.9443\n",
      "Epoch [77/800]: Train loss: 382.8784, Valid loss: 238.8263\n",
      "Epoch [78/800]: Train loss: 380.8059, Valid loss: 237.9628\n",
      "Epoch [79/800]: Train loss: 379.2194, Valid loss: 249.4081\n",
      "Epoch [80/800]: Train loss: 381.6063, Valid loss: 237.7865\n",
      "Epoch [81/800]: Train loss: 376.3189, Valid loss: 248.8760\n",
      "Epoch [82/800]: Train loss: 378.3420, Valid loss: 238.1867\n",
      "Epoch [83/800]: Train loss: 378.5792, Valid loss: 239.0486\n",
      "Epoch [84/800]: Train loss: 378.2828, Valid loss: 245.7560\n",
      "Epoch [85/800]: Train loss: 374.7199, Valid loss: 242.9436\n",
      "Epoch [86/800]: Train loss: 377.0485, Valid loss: 235.2361\n",
      "Saving model with loss 235.236...\n",
      "Epoch [87/800]: Train loss: 375.9977, Valid loss: 235.8736\n",
      "Epoch [88/800]: Train loss: 375.1401, Valid loss: 241.5975\n",
      "Epoch [89/800]: Train loss: 372.1789, Valid loss: 230.3980\n",
      "Saving model with loss 230.398...\n",
      "Epoch [90/800]: Train loss: 376.3047, Valid loss: 240.9042\n",
      "Epoch [91/800]: Train loss: 375.5394, Valid loss: 238.6989\n",
      "Epoch [92/800]: Train loss: 378.1093, Valid loss: 237.4295\n",
      "Epoch [93/800]: Train loss: 375.7544, Valid loss: 244.3239\n",
      "Epoch [94/800]: Train loss: 373.0325, Valid loss: 234.3390\n",
      "Epoch [95/800]: Train loss: 371.1238, Valid loss: 235.9422\n",
      "Epoch [96/800]: Train loss: 379.4131, Valid loss: 244.2469\n",
      "Epoch [97/800]: Train loss: 374.3575, Valid loss: 230.9550\n",
      "Epoch [98/800]: Train loss: 375.3649, Valid loss: 236.8899\n",
      "Epoch [99/800]: Train loss: 371.7467, Valid loss: 234.9612\n",
      "Epoch [100/800]: Train loss: 369.9200, Valid loss: 240.9401\n",
      "Epoch [101/800]: Train loss: 371.6874, Valid loss: 241.9585\n",
      "Epoch [102/800]: Train loss: 374.6782, Valid loss: 239.0322\n",
      "Epoch [103/800]: Train loss: 370.7882, Valid loss: 236.0465\n",
      "Epoch [104/800]: Train loss: 372.2243, Valid loss: 236.0652\n",
      "Epoch [105/800]: Train loss: 373.0995, Valid loss: 232.9518\n",
      "Epoch [106/800]: Train loss: 375.2234, Valid loss: 243.2302\n",
      "Epoch [107/800]: Train loss: 371.2869, Valid loss: 234.7486\n",
      "Epoch [108/800]: Train loss: 374.3498, Valid loss: 237.4698\n",
      "Epoch [109/800]: Train loss: 370.0963, Valid loss: 239.5598\n",
      "Epoch [110/800]: Train loss: 373.8812, Valid loss: 234.9677\n",
      "Epoch [111/800]: Train loss: 368.3900, Valid loss: 233.4624\n",
      "Epoch [112/800]: Train loss: 373.9525, Valid loss: 237.2720\n",
      "Epoch [113/800]: Train loss: 373.2474, Valid loss: 229.1072\n",
      "Saving model with loss 229.107...\n",
      "Epoch [114/800]: Train loss: 370.0389, Valid loss: 231.2774\n",
      "Epoch [115/800]: Train loss: 368.9675, Valid loss: 231.9465\n",
      "Epoch [116/800]: Train loss: 370.1079, Valid loss: 230.3638\n",
      "Epoch [117/800]: Train loss: 367.9677, Valid loss: 236.1998\n",
      "Epoch [118/800]: Train loss: 368.9241, Valid loss: 236.2867\n",
      "Epoch [119/800]: Train loss: 369.8015, Valid loss: 228.6322\n",
      "Saving model with loss 228.632...\n",
      "Epoch [120/800]: Train loss: 371.2519, Valid loss: 233.8751\n",
      "Epoch [121/800]: Train loss: 367.5109, Valid loss: 234.3220\n",
      "Epoch [122/800]: Train loss: 373.3318, Valid loss: 234.3308\n",
      "Epoch [123/800]: Train loss: 368.0407, Valid loss: 234.8769\n",
      "Epoch [124/800]: Train loss: 372.8499, Valid loss: 234.1008\n",
      "Epoch [125/800]: Train loss: 369.9693, Valid loss: 241.0443\n",
      "Epoch [126/800]: Train loss: 368.5767, Valid loss: 238.5125\n",
      "Epoch [127/800]: Train loss: 373.9442, Valid loss: 229.1409\n",
      "Epoch [128/800]: Train loss: 371.8146, Valid loss: 237.6807\n",
      "Epoch [129/800]: Train loss: 368.3638, Valid loss: 235.3316\n",
      "Epoch [130/800]: Train loss: 364.9888, Valid loss: 232.4417\n",
      "Epoch [131/800]: Train loss: 366.2815, Valid loss: 224.8496\n",
      "Saving model with loss 224.850...\n",
      "Epoch [132/800]: Train loss: 367.7896, Valid loss: 237.3495\n",
      "Epoch [133/800]: Train loss: 372.9801, Valid loss: 231.9010\n",
      "Epoch [134/800]: Train loss: 369.5805, Valid loss: 229.1953\n",
      "Epoch [135/800]: Train loss: 367.0819, Valid loss: 235.0613\n",
      "Epoch [136/800]: Train loss: 368.4296, Valid loss: 231.8086\n",
      "Epoch [137/800]: Train loss: 367.3413, Valid loss: 232.5838\n",
      "Epoch [138/800]: Train loss: 368.7890, Valid loss: 235.1752\n",
      "Epoch [139/800]: Train loss: 367.5586, Valid loss: 233.1064\n",
      "Epoch [140/800]: Train loss: 373.6596, Valid loss: 233.7632\n",
      "Epoch [141/800]: Train loss: 365.6628, Valid loss: 228.7577\n",
      "Epoch [142/800]: Train loss: 368.3200, Valid loss: 230.1470\n",
      "Epoch [143/800]: Train loss: 366.7940, Valid loss: 225.0665\n",
      "Epoch [144/800]: Train loss: 368.9913, Valid loss: 228.3442\n",
      "Epoch [145/800]: Train loss: 363.5412, Valid loss: 231.9124\n",
      "Epoch [146/800]: Train loss: 365.4473, Valid loss: 231.7905\n",
      "Epoch [147/800]: Train loss: 371.2871, Valid loss: 237.2534\n",
      "Epoch [148/800]: Train loss: 363.4780, Valid loss: 235.8090\n",
      "Epoch [149/800]: Train loss: 365.0954, Valid loss: 224.4653\n",
      "Saving model with loss 224.465...\n",
      "Epoch [150/800]: Train loss: 370.2533, Valid loss: 228.1545\n",
      "Epoch [151/800]: Train loss: 363.5374, Valid loss: 221.0618\n",
      "Saving model with loss 221.062...\n",
      "Epoch [152/800]: Train loss: 371.2837, Valid loss: 235.2251\n",
      "Epoch [153/800]: Train loss: 364.1879, Valid loss: 231.5538\n",
      "Epoch [154/800]: Train loss: 367.7611, Valid loss: 236.9819\n",
      "Epoch [155/800]: Train loss: 361.6778, Valid loss: 223.5462\n",
      "Epoch [156/800]: Train loss: 363.0777, Valid loss: 231.2790\n",
      "Epoch [157/800]: Train loss: 361.5347, Valid loss: 231.3887\n",
      "Epoch [158/800]: Train loss: 361.2673, Valid loss: 227.4065\n",
      "Epoch [159/800]: Train loss: 366.0688, Valid loss: 220.1804\n",
      "Saving model with loss 220.180...\n",
      "Epoch [160/800]: Train loss: 365.7380, Valid loss: 232.0783\n",
      "Epoch [161/800]: Train loss: 361.9041, Valid loss: 228.4744\n",
      "Epoch [162/800]: Train loss: 366.7312, Valid loss: 232.7388\n",
      "Epoch [163/800]: Train loss: 364.6532, Valid loss: 221.4467\n",
      "Epoch [164/800]: Train loss: 363.9093, Valid loss: 227.2927\n",
      "Epoch [165/800]: Train loss: 363.3372, Valid loss: 223.5675\n",
      "Epoch [166/800]: Train loss: 364.2553, Valid loss: 216.0810\n",
      "Saving model with loss 216.081...\n",
      "Epoch [167/800]: Train loss: 363.7967, Valid loss: 227.4751\n",
      "Epoch [168/800]: Train loss: 360.8341, Valid loss: 232.5026\n",
      "Epoch [169/800]: Train loss: 361.0193, Valid loss: 224.3108\n",
      "Epoch [170/800]: Train loss: 364.7075, Valid loss: 231.0337\n",
      "Epoch [171/800]: Train loss: 366.5227, Valid loss: 228.1656\n",
      "Epoch [172/800]: Train loss: 363.2330, Valid loss: 227.7386\n",
      "Epoch [173/800]: Train loss: 359.5702, Valid loss: 220.5818\n",
      "Epoch [174/800]: Train loss: 362.6663, Valid loss: 226.2495\n",
      "Epoch [175/800]: Train loss: 364.2895, Valid loss: 225.4936\n",
      "Epoch [176/800]: Train loss: 362.7719, Valid loss: 226.6759\n",
      "Epoch [177/800]: Train loss: 363.7797, Valid loss: 227.2008\n",
      "Epoch [178/800]: Train loss: 363.6646, Valid loss: 234.6549\n",
      "Epoch [179/800]: Train loss: 364.8235, Valid loss: 228.1353\n",
      "Epoch [180/800]: Train loss: 363.6349, Valid loss: 220.4554\n",
      "Epoch [181/800]: Train loss: 358.1841, Valid loss: 228.1558\n",
      "Epoch [182/800]: Train loss: 362.4238, Valid loss: 225.2147\n",
      "Epoch [183/800]: Train loss: 359.5364, Valid loss: 219.5655\n",
      "Epoch [184/800]: Train loss: 363.6139, Valid loss: 230.7932\n",
      "Epoch [185/800]: Train loss: 364.1789, Valid loss: 223.1344\n",
      "Epoch [186/800]: Train loss: 359.3610, Valid loss: 222.6978\n",
      "Epoch [187/800]: Train loss: 364.0959, Valid loss: 234.7782\n",
      "Epoch [188/800]: Train loss: 360.0926, Valid loss: 220.2963\n",
      "Epoch [189/800]: Train loss: 362.2361, Valid loss: 233.9503\n",
      "Epoch [190/800]: Train loss: 363.4913, Valid loss: 225.0498\n",
      "Epoch [191/800]: Train loss: 362.7005, Valid loss: 226.6517\n",
      "Epoch [192/800]: Train loss: 361.8528, Valid loss: 233.7119\n",
      "Epoch [193/800]: Train loss: 361.3218, Valid loss: 218.6903\n",
      "Epoch [194/800]: Train loss: 362.1746, Valid loss: 222.6840\n",
      "Epoch [195/800]: Train loss: 361.5923, Valid loss: 226.1285\n",
      "Epoch [196/800]: Train loss: 363.0064, Valid loss: 223.9379\n",
      "Epoch [197/800]: Train loss: 358.9286, Valid loss: 218.6365\n",
      "Epoch [198/800]: Train loss: 356.7804, Valid loss: 227.1121\n",
      "Epoch [199/800]: Train loss: 358.9302, Valid loss: 229.2620\n",
      "Epoch [200/800]: Train loss: 360.7101, Valid loss: 215.2777\n",
      "Saving model with loss 215.278...\n",
      "Epoch [201/800]: Train loss: 361.8674, Valid loss: 224.9491\n",
      "Epoch [202/800]: Train loss: 359.3671, Valid loss: 220.3581\n",
      "Epoch [203/800]: Train loss: 362.8382, Valid loss: 224.8582\n",
      "Epoch [204/800]: Train loss: 365.3324, Valid loss: 224.6731\n",
      "Epoch [205/800]: Train loss: 357.9031, Valid loss: 218.8728\n",
      "Epoch [206/800]: Train loss: 358.5993, Valid loss: 223.2091\n",
      "Epoch [207/800]: Train loss: 363.0735, Valid loss: 228.1302\n",
      "Epoch [208/800]: Train loss: 357.1583, Valid loss: 229.6135\n",
      "Epoch [209/800]: Train loss: 362.8001, Valid loss: 224.0969\n",
      "Epoch [210/800]: Train loss: 362.9177, Valid loss: 218.2362\n",
      "Epoch [211/800]: Train loss: 361.7782, Valid loss: 233.4630\n",
      "Epoch [212/800]: Train loss: 362.4587, Valid loss: 220.1349\n",
      "Epoch [213/800]: Train loss: 359.8831, Valid loss: 227.2972\n",
      "Epoch [214/800]: Train loss: 357.1403, Valid loss: 222.3218\n",
      "Epoch [215/800]: Train loss: 361.1628, Valid loss: 220.2353\n",
      "Epoch [216/800]: Train loss: 358.0964, Valid loss: 224.2582\n",
      "Epoch [217/800]: Train loss: 358.4655, Valid loss: 230.8431\n",
      "Epoch [218/800]: Train loss: 356.1902, Valid loss: 219.4989\n",
      "Epoch [219/800]: Train loss: 358.9231, Valid loss: 221.9518\n",
      "Epoch [220/800]: Train loss: 359.9056, Valid loss: 232.2384\n",
      "Epoch [221/800]: Train loss: 357.4667, Valid loss: 219.2134\n",
      "Epoch [222/800]: Train loss: 361.8402, Valid loss: 220.2984\n",
      "Epoch [223/800]: Train loss: 359.2446, Valid loss: 227.6061\n",
      "Epoch [224/800]: Train loss: 360.5891, Valid loss: 215.9843\n",
      "Epoch [225/800]: Train loss: 358.5391, Valid loss: 218.1730\n",
      "Epoch [226/800]: Train loss: 357.8239, Valid loss: 222.9834\n",
      "Epoch [227/800]: Train loss: 353.1529, Valid loss: 225.5010\n",
      "Epoch [228/800]: Train loss: 354.2553, Valid loss: 223.6145\n",
      "Epoch [229/800]: Train loss: 355.8325, Valid loss: 227.9955\n",
      "Epoch [230/800]: Train loss: 360.9272, Valid loss: 224.9873\n",
      "Epoch [231/800]: Train loss: 358.6204, Valid loss: 223.7787\n",
      "Epoch [232/800]: Train loss: 355.1262, Valid loss: 220.5652\n",
      "Epoch [233/800]: Train loss: 356.7792, Valid loss: 229.9523\n",
      "Epoch [234/800]: Train loss: 359.9900, Valid loss: 224.2780\n",
      "Epoch [235/800]: Train loss: 357.0824, Valid loss: 221.2421\n",
      "Epoch [236/800]: Train loss: 361.6958, Valid loss: 220.2511\n",
      "Epoch [237/800]: Train loss: 356.0534, Valid loss: 223.1611\n",
      "Epoch [238/800]: Train loss: 360.5929, Valid loss: 228.2077\n",
      "Epoch [239/800]: Train loss: 359.2942, Valid loss: 216.6212\n",
      "Epoch [240/800]: Train loss: 359.7352, Valid loss: 226.4219\n",
      "Epoch [241/800]: Train loss: 356.1362, Valid loss: 223.7150\n",
      "Epoch [242/800]: Train loss: 357.6923, Valid loss: 222.2761\n",
      "Epoch [243/800]: Train loss: 359.1396, Valid loss: 227.2874\n",
      "Epoch [244/800]: Train loss: 357.4960, Valid loss: 226.0272\n",
      "Epoch [245/800]: Train loss: 355.7124, Valid loss: 229.2932\n",
      "Epoch [246/800]: Train loss: 356.7762, Valid loss: 219.8369\n",
      "Epoch [247/800]: Train loss: 358.1125, Valid loss: 230.7851\n",
      "Epoch [248/800]: Train loss: 355.7959, Valid loss: 220.4692\n",
      "Epoch [249/800]: Train loss: 355.8154, Valid loss: 234.2355\n",
      "Epoch [250/800]: Train loss: 363.2621, Valid loss: 225.5493\n",
      "Epoch [251/800]: Train loss: 353.9612, Valid loss: 217.5187\n",
      "Epoch [252/800]: Train loss: 356.6882, Valid loss: 220.2842\n",
      "Epoch [253/800]: Train loss: 354.7786, Valid loss: 229.7130\n",
      "Epoch [254/800]: Train loss: 351.9248, Valid loss: 216.6109\n",
      "Epoch [255/800]: Train loss: 359.7638, Valid loss: 221.9994\n",
      "Epoch [256/800]: Train loss: 355.1424, Valid loss: 227.6011\n",
      "Epoch [257/800]: Train loss: 352.1950, Valid loss: 219.7228\n",
      "Epoch [258/800]: Train loss: 355.6487, Valid loss: 214.6262\n",
      "Saving model with loss 214.626...\n",
      "Epoch [259/800]: Train loss: 352.2833, Valid loss: 224.4396\n",
      "Epoch [260/800]: Train loss: 359.5149, Valid loss: 218.2648\n",
      "Epoch [261/800]: Train loss: 353.6277, Valid loss: 228.5605\n",
      "Epoch [262/800]: Train loss: 357.4149, Valid loss: 222.2104\n",
      "Epoch [263/800]: Train loss: 352.3087, Valid loss: 223.0545\n",
      "Epoch [264/800]: Train loss: 355.1812, Valid loss: 226.7787\n",
      "Epoch [265/800]: Train loss: 359.3991, Valid loss: 224.6471\n",
      "Epoch [266/800]: Train loss: 353.9753, Valid loss: 223.4553\n",
      "Epoch [267/800]: Train loss: 355.9638, Valid loss: 215.9202\n",
      "Epoch [268/800]: Train loss: 354.9926, Valid loss: 226.1472\n",
      "Epoch [269/800]: Train loss: 354.4367, Valid loss: 224.1047\n",
      "Epoch [270/800]: Train loss: 354.4756, Valid loss: 234.2138\n",
      "Epoch [271/800]: Train loss: 354.0747, Valid loss: 224.2144\n",
      "Epoch [272/800]: Train loss: 357.3633, Valid loss: 221.8089\n",
      "Epoch [273/800]: Train loss: 355.7814, Valid loss: 229.4246\n",
      "Epoch [274/800]: Train loss: 359.0831, Valid loss: 225.1464\n",
      "Epoch [275/800]: Train loss: 355.1874, Valid loss: 228.4086\n",
      "Epoch [276/800]: Train loss: 358.0145, Valid loss: 223.1379\n",
      "Epoch [277/800]: Train loss: 351.3118, Valid loss: 218.3290\n",
      "Epoch [278/800]: Train loss: 351.6921, Valid loss: 213.7893\n",
      "Saving model with loss 213.789...\n",
      "Epoch [279/800]: Train loss: 357.2244, Valid loss: 229.6916\n",
      "Epoch [280/800]: Train loss: 354.6457, Valid loss: 220.1869\n",
      "Epoch [281/800]: Train loss: 355.3519, Valid loss: 219.9517\n",
      "Epoch [282/800]: Train loss: 352.5028, Valid loss: 218.3532\n",
      "Epoch [283/800]: Train loss: 358.4648, Valid loss: 225.1144\n",
      "Epoch [284/800]: Train loss: 354.5239, Valid loss: 219.3621\n",
      "Epoch [285/800]: Train loss: 355.1495, Valid loss: 215.2498\n",
      "Epoch [286/800]: Train loss: 353.0095, Valid loss: 218.3923\n",
      "Epoch [287/800]: Train loss: 351.3793, Valid loss: 222.7621\n",
      "Epoch [288/800]: Train loss: 355.8719, Valid loss: 217.3214\n",
      "Epoch [289/800]: Train loss: 355.4654, Valid loss: 225.1441\n",
      "Epoch [290/800]: Train loss: 357.7546, Valid loss: 220.9506\n",
      "Epoch [291/800]: Train loss: 352.9693, Valid loss: 224.8499\n",
      "Epoch [292/800]: Train loss: 356.9148, Valid loss: 213.7466\n",
      "Saving model with loss 213.747...\n",
      "Epoch [293/800]: Train loss: 356.9541, Valid loss: 215.8012\n",
      "Epoch [294/800]: Train loss: 356.8929, Valid loss: 218.6395\n",
      "Epoch [295/800]: Train loss: 354.2075, Valid loss: 220.6146\n",
      "Epoch [296/800]: Train loss: 356.2155, Valid loss: 218.5720\n",
      "Epoch [297/800]: Train loss: 350.7444, Valid loss: 223.5760\n",
      "Epoch [298/800]: Train loss: 354.3033, Valid loss: 221.1833\n",
      "Epoch [299/800]: Train loss: 356.6150, Valid loss: 228.7745\n",
      "Epoch [300/800]: Train loss: 349.3209, Valid loss: 221.8058\n",
      "Epoch [301/800]: Train loss: 351.5233, Valid loss: 217.7674\n",
      "Epoch [302/800]: Train loss: 354.8372, Valid loss: 222.5077\n",
      "Epoch [303/800]: Train loss: 351.2191, Valid loss: 220.2399\n",
      "Epoch [304/800]: Train loss: 351.8668, Valid loss: 228.8879\n",
      "Epoch [305/800]: Train loss: 356.1207, Valid loss: 224.1024\n",
      "Epoch [306/800]: Train loss: 355.5537, Valid loss: 223.4478\n",
      "Epoch [307/800]: Train loss: 354.7607, Valid loss: 218.2098\n",
      "Epoch [308/800]: Train loss: 355.3722, Valid loss: 225.4262\n",
      "Epoch [309/800]: Train loss: 353.9443, Valid loss: 218.1866\n",
      "Epoch [310/800]: Train loss: 353.0550, Valid loss: 215.0636\n",
      "Epoch [311/800]: Train loss: 353.1463, Valid loss: 213.2545\n",
      "Saving model with loss 213.255...\n",
      "Epoch [312/800]: Train loss: 351.9832, Valid loss: 226.9844\n",
      "Epoch [313/800]: Train loss: 354.4169, Valid loss: 223.9034\n",
      "Epoch [314/800]: Train loss: 352.8786, Valid loss: 225.6725\n",
      "Epoch [315/800]: Train loss: 344.0812, Valid loss: 219.1508\n",
      "Epoch [316/800]: Train loss: 355.4953, Valid loss: 224.7306\n",
      "Epoch [317/800]: Train loss: 356.7111, Valid loss: 213.6950\n",
      "Epoch [318/800]: Train loss: 352.2672, Valid loss: 217.5877\n",
      "Epoch [319/800]: Train loss: 351.7338, Valid loss: 224.7347\n",
      "Epoch [320/800]: Train loss: 346.8171, Valid loss: 215.4598\n",
      "Epoch [321/800]: Train loss: 353.4756, Valid loss: 222.8049\n",
      "Epoch [322/800]: Train loss: 356.2348, Valid loss: 222.2833\n",
      "Epoch [323/800]: Train loss: 354.3419, Valid loss: 212.8272\n",
      "Saving model with loss 212.827...\n",
      "Epoch [324/800]: Train loss: 354.7372, Valid loss: 223.9849\n",
      "Epoch [325/800]: Train loss: 353.9191, Valid loss: 215.5549\n",
      "Epoch [326/800]: Train loss: 352.3596, Valid loss: 218.3691\n",
      "Epoch [327/800]: Train loss: 350.3708, Valid loss: 221.2849\n",
      "Epoch [328/800]: Train loss: 355.0181, Valid loss: 223.3310\n",
      "Epoch [329/800]: Train loss: 349.2925, Valid loss: 216.3174\n",
      "Epoch [330/800]: Train loss: 353.1281, Valid loss: 224.1991\n",
      "Epoch [331/800]: Train loss: 354.9251, Valid loss: 226.7317\n",
      "Epoch [332/800]: Train loss: 353.0047, Valid loss: 220.5373\n",
      "Epoch [333/800]: Train loss: 353.3344, Valid loss: 225.3250\n",
      "Epoch [334/800]: Train loss: 353.3509, Valid loss: 222.4629\n",
      "Epoch [335/800]: Train loss: 350.7731, Valid loss: 217.6784\n",
      "Epoch [336/800]: Train loss: 348.5508, Valid loss: 223.2765\n",
      "Epoch [337/800]: Train loss: 351.6428, Valid loss: 213.7843\n",
      "Epoch [338/800]: Train loss: 350.7521, Valid loss: 217.2458\n",
      "Epoch [339/800]: Train loss: 351.2050, Valid loss: 222.8809\n",
      "Epoch [340/800]: Train loss: 351.5297, Valid loss: 215.1959\n",
      "Epoch [341/800]: Train loss: 352.2470, Valid loss: 218.9746\n",
      "Epoch [342/800]: Train loss: 352.1489, Valid loss: 221.1897\n",
      "Epoch [343/800]: Train loss: 351.4775, Valid loss: 218.8182\n",
      "Epoch [344/800]: Train loss: 354.0134, Valid loss: 218.6079\n",
      "Epoch [345/800]: Train loss: 350.0018, Valid loss: 218.9656\n",
      "Epoch [346/800]: Train loss: 349.3087, Valid loss: 227.8564\n",
      "Epoch [347/800]: Train loss: 352.7347, Valid loss: 220.2452\n",
      "Epoch [348/800]: Train loss: 353.2581, Valid loss: 218.9498\n",
      "Epoch [349/800]: Train loss: 353.1091, Valid loss: 225.4714\n",
      "Epoch [350/800]: Train loss: 352.8097, Valid loss: 216.2899\n",
      "Epoch [351/800]: Train loss: 347.3556, Valid loss: 222.0503\n",
      "Epoch [352/800]: Train loss: 354.9369, Valid loss: 217.0801\n",
      "Epoch [353/800]: Train loss: 349.9346, Valid loss: 221.6843\n",
      "Epoch [354/800]: Train loss: 352.3463, Valid loss: 217.9438\n",
      "Epoch [355/800]: Train loss: 354.5144, Valid loss: 224.8070\n",
      "Epoch [356/800]: Train loss: 352.4686, Valid loss: 222.3328\n",
      "Epoch [357/800]: Train loss: 351.0707, Valid loss: 221.2155\n",
      "Epoch [358/800]: Train loss: 351.7760, Valid loss: 220.8032\n",
      "Epoch [359/800]: Train loss: 351.8385, Valid loss: 216.8708\n",
      "Epoch [360/800]: Train loss: 351.8013, Valid loss: 212.9301\n",
      "Epoch [361/800]: Train loss: 349.4681, Valid loss: 221.4869\n",
      "Epoch [362/800]: Train loss: 350.4054, Valid loss: 226.6061\n",
      "Epoch [363/800]: Train loss: 355.0408, Valid loss: 220.5408\n",
      "Epoch [364/800]: Train loss: 353.4343, Valid loss: 217.1399\n",
      "Epoch [365/800]: Train loss: 351.6515, Valid loss: 217.6682\n",
      "Epoch [366/800]: Train loss: 353.2176, Valid loss: 220.9805\n",
      "Epoch [367/800]: Train loss: 350.7190, Valid loss: 220.1851\n",
      "Epoch [368/800]: Train loss: 356.8613, Valid loss: 224.0396\n",
      "Epoch [369/800]: Train loss: 352.8036, Valid loss: 223.2947\n",
      "Epoch [370/800]: Train loss: 347.7611, Valid loss: 212.0180\n",
      "Saving model with loss 212.018...\n",
      "Epoch [371/800]: Train loss: 350.4861, Valid loss: 222.5944\n",
      "Epoch [372/800]: Train loss: 350.5655, Valid loss: 215.9474\n",
      "Epoch [373/800]: Train loss: 354.0187, Valid loss: 214.0253\n",
      "Epoch [374/800]: Train loss: 349.7635, Valid loss: 220.9078\n",
      "Epoch [375/800]: Train loss: 351.9617, Valid loss: 219.2043\n",
      "Epoch [376/800]: Train loss: 353.3965, Valid loss: 214.4661\n",
      "Epoch [377/800]: Train loss: 346.6923, Valid loss: 217.6430\n",
      "Epoch [378/800]: Train loss: 350.3830, Valid loss: 219.5670\n",
      "Epoch [379/800]: Train loss: 356.0244, Valid loss: 219.5361\n",
      "Epoch [380/800]: Train loss: 352.4716, Valid loss: 221.6712\n",
      "Epoch [381/800]: Train loss: 347.8646, Valid loss: 215.7818\n",
      "Epoch [382/800]: Train loss: 350.8248, Valid loss: 227.6563\n",
      "Epoch [383/800]: Train loss: 350.5077, Valid loss: 219.1541\n",
      "Epoch [384/800]: Train loss: 351.8704, Valid loss: 220.5365\n",
      "Epoch [385/800]: Train loss: 346.9761, Valid loss: 217.1435\n",
      "Epoch [386/800]: Train loss: 351.0350, Valid loss: 218.7378\n",
      "Epoch [387/800]: Train loss: 349.6149, Valid loss: 220.5932\n",
      "Epoch [388/800]: Train loss: 350.9993, Valid loss: 215.9154\n",
      "Epoch [389/800]: Train loss: 352.0192, Valid loss: 225.3615\n",
      "Epoch [390/800]: Train loss: 348.1824, Valid loss: 229.8989\n",
      "Epoch [391/800]: Train loss: 347.6955, Valid loss: 221.0526\n",
      "Epoch [392/800]: Train loss: 353.0590, Valid loss: 225.8094\n",
      "Epoch [393/800]: Train loss: 351.4189, Valid loss: 226.6591\n",
      "Epoch [394/800]: Train loss: 351.4991, Valid loss: 217.4731\n",
      "Epoch [395/800]: Train loss: 347.2637, Valid loss: 217.9970\n",
      "Epoch [396/800]: Train loss: 348.7945, Valid loss: 226.9184\n",
      "Epoch [397/800]: Train loss: 353.8743, Valid loss: 217.8821\n",
      "Epoch [398/800]: Train loss: 349.5243, Valid loss: 214.8614\n",
      "Epoch [399/800]: Train loss: 350.3612, Valid loss: 222.7729\n",
      "Epoch [400/800]: Train loss: 350.5998, Valid loss: 209.0101\n",
      "Saving model with loss 209.010...\n",
      "Epoch [401/800]: Train loss: 346.2809, Valid loss: 223.4288\n",
      "Epoch [402/800]: Train loss: 353.0429, Valid loss: 212.3596\n",
      "Epoch [403/800]: Train loss: 354.5032, Valid loss: 217.4152\n",
      "Epoch [404/800]: Train loss: 350.1612, Valid loss: 221.1434\n",
      "Epoch [405/800]: Train loss: 346.4248, Valid loss: 223.7020\n",
      "Epoch [406/800]: Train loss: 347.6762, Valid loss: 217.3765\n",
      "Epoch [407/800]: Train loss: 354.9034, Valid loss: 225.7926\n",
      "Epoch [408/800]: Train loss: 347.9948, Valid loss: 221.5648\n",
      "Epoch [409/800]: Train loss: 352.6766, Valid loss: 214.6093\n",
      "Epoch [410/800]: Train loss: 351.2631, Valid loss: 224.1516\n",
      "Epoch [411/800]: Train loss: 351.8207, Valid loss: 222.0311\n",
      "Epoch [412/800]: Train loss: 353.4343, Valid loss: 220.8773\n",
      "Epoch [413/800]: Train loss: 352.5522, Valid loss: 227.5821\n",
      "Epoch [414/800]: Train loss: 350.7635, Valid loss: 219.7851\n",
      "Epoch [415/800]: Train loss: 345.5637, Valid loss: 223.4455\n",
      "Epoch [416/800]: Train loss: 353.0256, Valid loss: 226.9677\n",
      "Epoch [417/800]: Train loss: 351.2095, Valid loss: 221.2687\n",
      "Epoch [418/800]: Train loss: 348.9021, Valid loss: 216.3818\n",
      "Epoch [419/800]: Train loss: 349.9477, Valid loss: 219.1488\n",
      "Epoch [420/800]: Train loss: 348.4904, Valid loss: 226.3613\n",
      "Epoch [421/800]: Train loss: 348.2918, Valid loss: 214.6039\n",
      "Epoch [422/800]: Train loss: 346.3321, Valid loss: 222.7278\n",
      "Epoch [423/800]: Train loss: 349.6136, Valid loss: 217.7486\n",
      "Epoch [424/800]: Train loss: 348.0479, Valid loss: 217.3270\n",
      "Epoch [425/800]: Train loss: 350.1035, Valid loss: 216.6411\n",
      "Epoch [426/800]: Train loss: 352.2824, Valid loss: 219.9264\n",
      "Epoch [427/800]: Train loss: 347.8427, Valid loss: 219.0725\n",
      "Epoch [428/800]: Train loss: 347.4848, Valid loss: 217.2219\n",
      "Epoch [429/800]: Train loss: 352.5585, Valid loss: 222.9324\n",
      "Epoch [430/800]: Train loss: 346.8844, Valid loss: 219.9193\n",
      "Epoch [431/800]: Train loss: 351.3151, Valid loss: 210.4216\n",
      "Epoch [432/800]: Train loss: 348.5156, Valid loss: 227.3030\n",
      "Epoch [433/800]: Train loss: 346.8788, Valid loss: 217.3345\n",
      "Epoch [434/800]: Train loss: 348.3344, Valid loss: 225.6742\n",
      "Epoch [435/800]: Train loss: 352.1871, Valid loss: 222.2928\n",
      "Epoch [436/800]: Train loss: 348.2310, Valid loss: 221.7951\n",
      "Epoch [437/800]: Train loss: 346.5628, Valid loss: 221.6590\n",
      "Epoch [438/800]: Train loss: 352.1832, Valid loss: 215.5393\n",
      "Epoch [439/800]: Train loss: 348.5955, Valid loss: 222.7912\n",
      "Epoch [440/800]: Train loss: 348.5650, Valid loss: 214.8483\n",
      "Epoch [441/800]: Train loss: 347.3287, Valid loss: 219.4465\n",
      "Epoch [442/800]: Train loss: 351.0174, Valid loss: 216.0678\n",
      "Epoch [443/800]: Train loss: 348.0271, Valid loss: 225.7424\n",
      "Epoch [444/800]: Train loss: 350.1546, Valid loss: 221.8387\n",
      "Epoch [445/800]: Train loss: 347.0373, Valid loss: 223.5735\n",
      "Epoch [446/800]: Train loss: 350.5066, Valid loss: 220.3486\n",
      "Epoch [447/800]: Train loss: 348.0166, Valid loss: 216.2496\n",
      "Epoch [448/800]: Train loss: 347.6186, Valid loss: 222.2935\n",
      "Epoch [449/800]: Train loss: 350.1717, Valid loss: 220.9964\n",
      "Epoch [450/800]: Train loss: 346.6368, Valid loss: 219.7566\n",
      "Epoch [451/800]: Train loss: 348.3945, Valid loss: 212.7301\n",
      "Epoch [452/800]: Train loss: 345.8548, Valid loss: 209.9920\n",
      "Epoch [453/800]: Train loss: 348.8733, Valid loss: 216.9180\n",
      "Epoch [454/800]: Train loss: 346.9901, Valid loss: 214.6318\n",
      "Epoch [455/800]: Train loss: 346.2811, Valid loss: 223.1503\n",
      "Epoch [456/800]: Train loss: 346.9268, Valid loss: 220.2261\n",
      "Epoch [457/800]: Train loss: 351.6022, Valid loss: 221.8719\n",
      "Epoch [458/800]: Train loss: 346.7455, Valid loss: 221.4183\n",
      "Epoch [459/800]: Train loss: 348.8102, Valid loss: 218.0271\n",
      "Epoch [460/800]: Train loss: 346.0538, Valid loss: 217.4144\n",
      "Epoch [461/800]: Train loss: 347.8637, Valid loss: 223.1421\n",
      "Epoch [462/800]: Train loss: 348.5184, Valid loss: 216.3805\n",
      "Epoch [463/800]: Train loss: 347.4965, Valid loss: 221.9787\n",
      "Epoch [464/800]: Train loss: 349.1857, Valid loss: 216.8418\n",
      "Epoch [465/800]: Train loss: 350.9850, Valid loss: 217.9600\n",
      "Epoch [466/800]: Train loss: 352.0760, Valid loss: 223.4812\n",
      "Epoch [467/800]: Train loss: 346.5908, Valid loss: 217.5398\n",
      "Epoch [468/800]: Train loss: 350.9571, Valid loss: 220.6042\n",
      "Epoch [469/800]: Train loss: 349.8951, Valid loss: 217.2252\n",
      "Epoch [470/800]: Train loss: 349.5873, Valid loss: 220.9628\n",
      "Epoch [471/800]: Train loss: 346.0812, Valid loss: 225.4735\n",
      "Epoch [472/800]: Train loss: 345.6253, Valid loss: 227.3667\n",
      "Epoch [473/800]: Train loss: 350.5781, Valid loss: 217.7265\n",
      "Epoch [474/800]: Train loss: 349.4832, Valid loss: 214.3450\n",
      "Epoch [475/800]: Train loss: 348.4799, Valid loss: 222.4669\n",
      "Epoch [476/800]: Train loss: 351.7704, Valid loss: 217.0129\n",
      "Epoch [477/800]: Train loss: 351.0545, Valid loss: 216.2731\n",
      "Epoch [478/800]: Train loss: 348.8901, Valid loss: 215.7500\n",
      "Epoch [479/800]: Train loss: 347.0980, Valid loss: 215.8437\n",
      "Epoch [480/800]: Train loss: 347.4177, Valid loss: 214.3660\n",
      "Epoch [481/800]: Train loss: 347.7376, Valid loss: 216.6803\n",
      "Epoch [482/800]: Train loss: 350.3438, Valid loss: 216.5982\n",
      "Epoch [483/800]: Train loss: 350.8072, Valid loss: 217.9442\n",
      "Epoch [484/800]: Train loss: 350.0133, Valid loss: 224.5980\n",
      "Epoch [485/800]: Train loss: 347.8265, Valid loss: 212.7828\n",
      "Epoch [486/800]: Train loss: 347.0150, Valid loss: 215.4996\n",
      "Epoch [487/800]: Train loss: 349.8677, Valid loss: 213.6566\n",
      "Epoch [488/800]: Train loss: 344.1498, Valid loss: 220.3155\n",
      "Epoch [489/800]: Train loss: 348.4311, Valid loss: 213.4432\n",
      "Epoch [490/800]: Train loss: 350.0031, Valid loss: 218.8992\n",
      "Epoch [491/800]: Train loss: 351.3779, Valid loss: 221.8815\n",
      "Epoch [492/800]: Train loss: 348.2310, Valid loss: 217.6996\n",
      "Epoch [493/800]: Train loss: 350.2290, Valid loss: 221.5135\n",
      "Epoch [494/800]: Train loss: 350.8370, Valid loss: 219.8423\n",
      "Epoch [495/800]: Train loss: 354.2394, Valid loss: 216.3270\n",
      "Epoch [496/800]: Train loss: 346.7359, Valid loss: 217.1678\n",
      "Epoch [497/800]: Train loss: 347.9497, Valid loss: 220.5602\n",
      "Epoch [498/800]: Train loss: 350.1697, Valid loss: 217.8365\n",
      "Epoch [499/800]: Train loss: 350.8587, Valid loss: 219.3733\n",
      "Epoch [500/800]: Train loss: 347.6898, Valid loss: 210.7810\n",
      "Epoch [501/800]: Train loss: 348.8380, Valid loss: 216.0560\n",
      "Epoch [502/800]: Train loss: 351.1425, Valid loss: 212.8497\n",
      "Epoch [503/800]: Train loss: 351.3524, Valid loss: 219.5290\n",
      "Epoch [504/800]: Train loss: 349.7240, Valid loss: 225.6098\n",
      "Epoch [505/800]: Train loss: 350.9638, Valid loss: 221.0381\n",
      "Epoch [506/800]: Train loss: 352.0048, Valid loss: 224.9625\n",
      "Epoch [507/800]: Train loss: 349.6464, Valid loss: 216.9285\n",
      "Epoch [508/800]: Train loss: 350.9422, Valid loss: 230.4196\n",
      "Epoch [509/800]: Train loss: 348.8571, Valid loss: 219.0535\n",
      "Epoch [510/800]: Train loss: 349.9046, Valid loss: 216.5892\n",
      "Epoch [511/800]: Train loss: 347.9351, Valid loss: 226.1457\n",
      "Epoch [512/800]: Train loss: 346.1558, Valid loss: 223.3580\n",
      "Epoch [513/800]: Train loss: 350.8387, Valid loss: 211.0567\n",
      "Epoch [514/800]: Train loss: 350.3658, Valid loss: 220.6513\n",
      "Epoch [515/800]: Train loss: 350.2985, Valid loss: 220.1086\n",
      "Epoch [516/800]: Train loss: 349.1564, Valid loss: 217.2814\n",
      "Epoch [517/800]: Train loss: 352.1551, Valid loss: 218.8246\n",
      "Epoch [518/800]: Train loss: 351.0357, Valid loss: 212.9257\n",
      "Epoch [519/800]: Train loss: 348.4328, Valid loss: 213.0495\n",
      "Epoch [520/800]: Train loss: 348.0591, Valid loss: 227.7129\n",
      "Epoch [521/800]: Train loss: 343.1081, Valid loss: 219.3506\n",
      "Epoch [522/800]: Train loss: 350.2257, Valid loss: 225.1878\n",
      "Epoch [523/800]: Train loss: 347.4395, Valid loss: 215.1833\n",
      "Epoch [524/800]: Train loss: 349.6796, Valid loss: 214.9854\n",
      "Epoch [525/800]: Train loss: 345.7724, Valid loss: 216.8800\n",
      "Epoch [526/800]: Train loss: 352.4686, Valid loss: 216.5562\n",
      "Epoch [527/800]: Train loss: 346.4821, Valid loss: 216.5460\n",
      "Epoch [528/800]: Train loss: 350.7976, Valid loss: 218.4814\n",
      "Epoch [529/800]: Train loss: 347.8066, Valid loss: 223.6099\n",
      "Epoch [530/800]: Train loss: 348.3169, Valid loss: 217.6442\n",
      "Epoch [531/800]: Train loss: 353.3024, Valid loss: 221.6048\n",
      "Epoch [532/800]: Train loss: 347.5136, Valid loss: 218.0433\n",
      "Epoch [533/800]: Train loss: 348.3355, Valid loss: 214.2868\n",
      "Epoch [534/800]: Train loss: 348.8382, Valid loss: 221.7754\n",
      "Epoch [535/800]: Train loss: 345.9705, Valid loss: 211.1741\n",
      "Epoch [536/800]: Train loss: 346.0864, Valid loss: 214.9999\n",
      "Epoch [537/800]: Train loss: 348.5083, Valid loss: 218.2017\n",
      "Epoch [538/800]: Train loss: 347.8307, Valid loss: 216.4230\n",
      "Epoch [539/800]: Train loss: 347.3825, Valid loss: 219.0023\n",
      "Epoch [540/800]: Train loss: 348.3698, Valid loss: 223.3857\n",
      "Epoch [541/800]: Train loss: 355.1332, Valid loss: 213.6129\n",
      "Epoch [542/800]: Train loss: 348.0366, Valid loss: 214.9959\n",
      "Epoch [543/800]: Train loss: 348.5831, Valid loss: 212.7140\n",
      "Epoch [544/800]: Train loss: 351.8009, Valid loss: 223.2137\n",
      "Epoch [545/800]: Train loss: 348.0514, Valid loss: 220.0084\n",
      "Epoch [546/800]: Train loss: 347.9111, Valid loss: 212.5141\n",
      "Epoch [547/800]: Train loss: 349.5051, Valid loss: 214.1332\n",
      "Epoch [548/800]: Train loss: 345.2165, Valid loss: 211.9115\n",
      "Epoch [549/800]: Train loss: 342.5829, Valid loss: 218.1076\n",
      "Epoch [550/800]: Train loss: 348.0719, Valid loss: 221.4067\n",
      "Epoch [551/800]: Train loss: 348.6386, Valid loss: 216.0596\n",
      "Epoch [552/800]: Train loss: 347.9431, Valid loss: 217.4749\n",
      "Epoch [553/800]: Train loss: 345.1397, Valid loss: 221.4552\n",
      "Epoch [554/800]: Train loss: 342.9419, Valid loss: 217.4753\n",
      "Epoch [555/800]: Train loss: 350.0806, Valid loss: 213.7772\n",
      "Epoch [556/800]: Train loss: 343.2851, Valid loss: 217.2905\n",
      "Epoch [557/800]: Train loss: 351.3726, Valid loss: 216.2007\n",
      "Epoch [558/800]: Train loss: 345.7310, Valid loss: 222.4218\n",
      "Epoch [559/800]: Train loss: 347.8168, Valid loss: 213.8924\n",
      "Epoch [560/800]: Train loss: 348.5952, Valid loss: 215.9371\n",
      "Epoch [561/800]: Train loss: 349.8946, Valid loss: 229.1800\n",
      "Epoch [562/800]: Train loss: 347.1176, Valid loss: 212.3985\n",
      "Epoch [563/800]: Train loss: 346.5864, Valid loss: 211.7062\n",
      "Epoch [564/800]: Train loss: 351.5506, Valid loss: 219.4431\n",
      "Epoch [565/800]: Train loss: 346.6525, Valid loss: 211.7502\n",
      "Epoch [566/800]: Train loss: 352.9297, Valid loss: 213.6739\n",
      "Epoch [567/800]: Train loss: 347.9396, Valid loss: 219.6486\n",
      "Epoch [568/800]: Train loss: 346.1626, Valid loss: 213.4006\n",
      "Epoch [569/800]: Train loss: 350.8981, Valid loss: 222.6103\n",
      "Epoch [570/800]: Train loss: 346.4281, Valid loss: 214.7251\n",
      "Epoch [571/800]: Train loss: 348.1602, Valid loss: 223.9842\n",
      "Epoch [572/800]: Train loss: 346.7244, Valid loss: 215.8572\n",
      "Epoch [573/800]: Train loss: 347.5072, Valid loss: 215.8169\n",
      "Epoch [574/800]: Train loss: 349.3819, Valid loss: 221.8138\n",
      "Epoch [575/800]: Train loss: 346.5785, Valid loss: 225.5092\n",
      "Epoch [576/800]: Train loss: 343.7606, Valid loss: 205.8336\n",
      "Saving model with loss 205.834...\n",
      "Epoch [577/800]: Train loss: 350.0478, Valid loss: 213.6612\n",
      "Epoch [578/800]: Train loss: 346.7564, Valid loss: 220.6635\n",
      "Epoch [579/800]: Train loss: 346.4359, Valid loss: 218.0844\n",
      "Epoch [580/800]: Train loss: 344.2366, Valid loss: 211.8435\n",
      "Epoch [581/800]: Train loss: 345.5438, Valid loss: 222.1907\n",
      "Epoch [582/800]: Train loss: 350.3142, Valid loss: 219.8746\n",
      "Epoch [583/800]: Train loss: 347.3161, Valid loss: 214.1232\n",
      "Epoch [584/800]: Train loss: 350.6833, Valid loss: 216.9925\n",
      "Epoch [585/800]: Train loss: 346.3315, Valid loss: 210.5208\n",
      "Epoch [586/800]: Train loss: 347.2542, Valid loss: 215.4671\n",
      "Epoch [587/800]: Train loss: 347.7259, Valid loss: 221.7426\n",
      "Epoch [588/800]: Train loss: 347.7660, Valid loss: 229.3958\n",
      "Epoch [589/800]: Train loss: 344.5636, Valid loss: 213.8235\n",
      "Epoch [590/800]: Train loss: 344.2768, Valid loss: 218.9035\n",
      "Epoch [591/800]: Train loss: 345.6950, Valid loss: 222.7343\n",
      "Epoch [592/800]: Train loss: 346.2407, Valid loss: 219.6056\n",
      "Epoch [593/800]: Train loss: 348.6162, Valid loss: 216.5730\n",
      "Epoch [594/800]: Train loss: 347.7914, Valid loss: 211.1616\n",
      "Epoch [595/800]: Train loss: 347.4893, Valid loss: 220.6633\n",
      "Epoch [596/800]: Train loss: 351.8319, Valid loss: 218.4322\n",
      "Epoch [597/800]: Train loss: 344.1649, Valid loss: 208.1651\n",
      "Epoch [598/800]: Train loss: 347.4041, Valid loss: 218.9700\n",
      "Epoch [599/800]: Train loss: 348.4644, Valid loss: 214.4905\n",
      "Epoch [600/800]: Train loss: 346.2833, Valid loss: 213.8786\n",
      "Epoch [601/800]: Train loss: 345.6074, Valid loss: 216.9463\n",
      "Epoch [602/800]: Train loss: 345.8935, Valid loss: 217.5822\n",
      "Epoch [603/800]: Train loss: 349.3069, Valid loss: 214.0663\n",
      "Epoch [604/800]: Train loss: 344.3692, Valid loss: 217.2348\n",
      "Epoch [605/800]: Train loss: 348.6256, Valid loss: 215.5225\n",
      "Epoch [606/800]: Train loss: 346.8491, Valid loss: 214.3773\n",
      "Epoch [607/800]: Train loss: 344.6032, Valid loss: 215.4626\n",
      "Epoch [608/800]: Train loss: 353.8597, Valid loss: 215.6877\n",
      "Epoch [609/800]: Train loss: 342.9267, Valid loss: 212.8694\n",
      "Epoch [610/800]: Train loss: 346.7661, Valid loss: 212.0322\n",
      "Epoch [611/800]: Train loss: 347.7060, Valid loss: 216.8109\n",
      "Epoch [612/800]: Train loss: 343.2383, Valid loss: 220.0070\n",
      "Epoch [613/800]: Train loss: 348.3130, Valid loss: 211.3325\n",
      "Epoch [614/800]: Train loss: 344.8981, Valid loss: 214.7426\n",
      "Epoch [615/800]: Train loss: 345.7209, Valid loss: 213.8678\n",
      "Epoch [616/800]: Train loss: 345.2429, Valid loss: 217.9723\n",
      "Epoch [617/800]: Train loss: 348.8945, Valid loss: 216.1513\n",
      "Epoch [618/800]: Train loss: 351.4942, Valid loss: 214.7090\n",
      "Epoch [619/800]: Train loss: 351.3737, Valid loss: 224.2286\n",
      "Epoch [620/800]: Train loss: 344.4032, Valid loss: 220.1319\n",
      "Epoch [621/800]: Train loss: 347.2891, Valid loss: 213.1735\n",
      "Epoch [622/800]: Train loss: 346.1191, Valid loss: 211.4031\n",
      "Epoch [623/800]: Train loss: 344.7697, Valid loss: 215.3394\n",
      "Epoch [624/800]: Train loss: 346.1547, Valid loss: 211.6835\n",
      "Epoch [625/800]: Train loss: 347.3497, Valid loss: 213.9273\n",
      "Epoch [626/800]: Train loss: 347.7203, Valid loss: 217.5125\n",
      "Epoch [627/800]: Train loss: 351.8831, Valid loss: 219.0020\n",
      "Epoch [628/800]: Train loss: 344.2597, Valid loss: 213.1144\n",
      "Epoch [629/800]: Train loss: 347.4349, Valid loss: 211.7300\n",
      "Epoch [630/800]: Train loss: 350.0787, Valid loss: 216.1109\n",
      "Epoch [631/800]: Train loss: 343.6704, Valid loss: 222.5031\n",
      "Epoch [632/800]: Train loss: 344.9458, Valid loss: 212.6694\n",
      "Epoch [633/800]: Train loss: 347.1354, Valid loss: 216.6445\n",
      "Epoch [634/800]: Train loss: 346.9341, Valid loss: 212.6084\n",
      "Epoch [635/800]: Train loss: 347.8770, Valid loss: 216.4603\n",
      "Epoch [636/800]: Train loss: 348.6242, Valid loss: 214.0553\n",
      "Epoch [637/800]: Train loss: 352.7009, Valid loss: 214.7831\n",
      "Epoch [638/800]: Train loss: 348.0837, Valid loss: 220.7352\n",
      "Epoch [639/800]: Train loss: 347.9919, Valid loss: 224.3346\n",
      "Epoch [640/800]: Train loss: 346.6891, Valid loss: 215.4292\n",
      "Epoch [641/800]: Train loss: 342.3445, Valid loss: 218.1525\n",
      "Epoch [642/800]: Train loss: 347.2248, Valid loss: 207.1209\n",
      "Epoch [643/800]: Train loss: 343.2061, Valid loss: 216.1644\n",
      "Epoch [644/800]: Train loss: 345.5175, Valid loss: 219.7458\n",
      "Epoch [645/800]: Train loss: 348.4055, Valid loss: 212.9247\n",
      "Epoch [646/800]: Train loss: 348.9368, Valid loss: 223.2227\n",
      "Epoch [647/800]: Train loss: 348.6270, Valid loss: 208.9016\n",
      "Epoch [648/800]: Train loss: 346.0127, Valid loss: 221.6559\n",
      "Epoch [649/800]: Train loss: 346.9932, Valid loss: 213.7327\n",
      "Epoch [650/800]: Train loss: 344.7451, Valid loss: 215.9917\n",
      "Epoch [651/800]: Train loss: 346.6615, Valid loss: 214.5263\n",
      "Epoch [652/800]: Train loss: 345.9933, Valid loss: 212.2748\n",
      "Epoch [653/800]: Train loss: 347.4703, Valid loss: 216.7105\n",
      "Epoch [654/800]: Train loss: 346.7907, Valid loss: 211.1128\n",
      "Epoch [655/800]: Train loss: 347.4273, Valid loss: 223.2631\n",
      "Epoch [656/800]: Train loss: 348.8833, Valid loss: 216.7435\n",
      "Epoch [657/800]: Train loss: 348.7063, Valid loss: 209.6272\n",
      "Epoch [658/800]: Train loss: 346.1822, Valid loss: 214.4764\n",
      "Epoch [659/800]: Train loss: 346.7594, Valid loss: 214.4564\n",
      "Epoch [660/800]: Train loss: 344.1758, Valid loss: 220.9027\n",
      "Epoch [661/800]: Train loss: 347.2318, Valid loss: 216.2621\n",
      "Epoch [662/800]: Train loss: 343.3749, Valid loss: 220.1059\n",
      "Epoch [663/800]: Train loss: 346.0237, Valid loss: 223.3890\n",
      "Epoch [664/800]: Train loss: 350.4325, Valid loss: 214.6976\n",
      "Epoch [665/800]: Train loss: 345.8346, Valid loss: 217.0468\n",
      "Epoch [666/800]: Train loss: 347.9267, Valid loss: 211.7364\n",
      "Epoch [667/800]: Train loss: 345.0309, Valid loss: 214.8684\n",
      "Epoch [668/800]: Train loss: 342.3383, Valid loss: 210.3645\n",
      "Epoch [669/800]: Train loss: 345.4330, Valid loss: 220.3046\n",
      "Epoch [670/800]: Train loss: 344.8783, Valid loss: 216.5400\n",
      "Epoch [671/800]: Train loss: 347.7411, Valid loss: 212.7245\n",
      "Epoch [672/800]: Train loss: 344.8982, Valid loss: 211.3259\n",
      "Epoch [673/800]: Train loss: 345.3889, Valid loss: 224.5174\n",
      "Epoch [674/800]: Train loss: 345.4401, Valid loss: 218.0222\n",
      "Epoch [675/800]: Train loss: 340.6520, Valid loss: 210.8440\n",
      "Epoch [676/800]: Train loss: 346.8569, Valid loss: 219.0082\n",
      "Epoch [677/800]: Train loss: 349.0975, Valid loss: 219.7021\n",
      "Epoch [678/800]: Train loss: 348.9465, Valid loss: 212.9851\n",
      "Epoch [679/800]: Train loss: 344.9944, Valid loss: 214.2757\n",
      "Epoch [680/800]: Train loss: 343.2222, Valid loss: 213.6685\n",
      "Epoch [681/800]: Train loss: 346.0600, Valid loss: 222.0738\n",
      "Epoch [682/800]: Train loss: 348.2575, Valid loss: 217.4944\n",
      "Epoch [683/800]: Train loss: 344.5608, Valid loss: 214.7338\n",
      "Epoch [684/800]: Train loss: 348.8167, Valid loss: 213.9649\n",
      "Epoch [685/800]: Train loss: 347.6318, Valid loss: 222.2059\n",
      "Epoch [686/800]: Train loss: 346.6326, Valid loss: 214.3301\n",
      "Epoch [687/800]: Train loss: 348.1941, Valid loss: 217.2371\n",
      "Epoch [688/800]: Train loss: 344.5445, Valid loss: 207.4616\n",
      "Epoch [689/800]: Train loss: 346.2432, Valid loss: 217.4116\n",
      "Epoch [690/800]: Train loss: 347.0034, Valid loss: 215.5262\n",
      "Epoch [691/800]: Train loss: 348.0433, Valid loss: 213.5214\n",
      "Epoch [692/800]: Train loss: 347.9249, Valid loss: 212.9296\n",
      "Epoch [693/800]: Train loss: 353.0262, Valid loss: 220.6002\n",
      "Epoch [694/800]: Train loss: 344.4754, Valid loss: 218.0957\n",
      "Epoch [695/800]: Train loss: 348.7778, Valid loss: 214.5611\n",
      "Epoch [696/800]: Train loss: 350.2542, Valid loss: 227.4569\n",
      "Epoch [697/800]: Train loss: 347.1666, Valid loss: 220.6958\n",
      "Epoch [698/800]: Train loss: 346.8619, Valid loss: 215.6075\n",
      "Epoch [699/800]: Train loss: 347.5612, Valid loss: 215.1408\n",
      "Epoch [700/800]: Train loss: 343.6489, Valid loss: 212.3490\n",
      "Epoch [701/800]: Train loss: 348.9733, Valid loss: 220.6400\n",
      "Epoch [702/800]: Train loss: 343.7959, Valid loss: 209.1496\n",
      "Epoch [703/800]: Train loss: 347.3422, Valid loss: 221.9900\n",
      "Epoch [704/800]: Train loss: 348.5107, Valid loss: 214.6935\n",
      "Epoch [705/800]: Train loss: 346.5818, Valid loss: 215.6834\n",
      "Epoch [706/800]: Train loss: 341.5475, Valid loss: 213.7049\n",
      "Epoch [707/800]: Train loss: 347.3917, Valid loss: 216.6625\n",
      "Epoch [708/800]: Train loss: 342.9449, Valid loss: 211.6764\n",
      "Epoch [709/800]: Train loss: 347.6977, Valid loss: 209.8965\n",
      "Epoch [710/800]: Train loss: 346.0703, Valid loss: 219.9643\n",
      "Epoch [711/800]: Train loss: 350.0252, Valid loss: 215.2217\n",
      "Epoch [712/800]: Train loss: 346.3172, Valid loss: 214.2614\n",
      "Epoch [713/800]: Train loss: 347.7896, Valid loss: 221.8818\n",
      "Epoch [714/800]: Train loss: 347.4070, Valid loss: 214.9914\n",
      "Epoch [715/800]: Train loss: 347.9309, Valid loss: 215.6258\n",
      "Epoch [716/800]: Train loss: 347.7463, Valid loss: 215.8493\n",
      "Epoch [717/800]: Train loss: 349.0621, Valid loss: 206.9825\n",
      "Epoch [718/800]: Train loss: 348.2492, Valid loss: 215.7693\n",
      "Epoch [719/800]: Train loss: 346.8474, Valid loss: 210.8272\n",
      "Epoch [720/800]: Train loss: 349.2087, Valid loss: 212.4351\n",
      "Epoch [721/800]: Train loss: 347.5257, Valid loss: 218.6654\n",
      "Epoch [722/800]: Train loss: 347.5141, Valid loss: 214.8664\n",
      "Epoch [723/800]: Train loss: 347.0766, Valid loss: 222.4448\n",
      "Epoch [724/800]: Train loss: 343.1187, Valid loss: 206.9098\n",
      "Epoch [725/800]: Train loss: 341.5768, Valid loss: 219.5955\n",
      "Epoch [726/800]: Train loss: 347.6872, Valid loss: 226.6326\n",
      "Epoch [727/800]: Train loss: 349.0594, Valid loss: 215.4267\n",
      "Epoch [728/800]: Train loss: 344.8134, Valid loss: 211.3626\n",
      "Epoch [729/800]: Train loss: 347.5052, Valid loss: 218.3391\n",
      "Epoch [730/800]: Train loss: 344.7876, Valid loss: 214.3269\n",
      "Epoch [731/800]: Train loss: 344.9829, Valid loss: 207.1778\n",
      "Epoch [732/800]: Train loss: 344.7538, Valid loss: 211.2911\n",
      "Epoch [733/800]: Train loss: 346.0796, Valid loss: 220.2298\n",
      "Epoch [734/800]: Train loss: 344.2865, Valid loss: 218.1687\n",
      "Epoch [735/800]: Train loss: 340.1674, Valid loss: 213.4414\n",
      "Epoch [736/800]: Train loss: 347.7099, Valid loss: 221.3447\n",
      "Epoch [737/800]: Train loss: 347.3391, Valid loss: 211.3068\n",
      "Epoch [738/800]: Train loss: 344.2114, Valid loss: 218.2112\n",
      "Epoch [739/800]: Train loss: 345.0017, Valid loss: 214.6265\n",
      "Epoch [740/800]: Train loss: 344.3373, Valid loss: 218.8519\n",
      "Epoch [741/800]: Train loss: 346.6963, Valid loss: 218.2184\n",
      "Epoch [742/800]: Train loss: 343.1691, Valid loss: 213.5673\n",
      "Epoch [743/800]: Train loss: 350.8664, Valid loss: 212.9612\n",
      "Epoch [744/800]: Train loss: 347.2899, Valid loss: 218.6661\n",
      "Epoch [745/800]: Train loss: 345.4556, Valid loss: 208.7847\n",
      "Epoch [746/800]: Train loss: 346.9169, Valid loss: 216.4670\n",
      "Epoch [747/800]: Train loss: 348.8878, Valid loss: 224.1097\n",
      "Epoch [748/800]: Train loss: 345.1969, Valid loss: 214.7997\n",
      "Epoch [749/800]: Train loss: 348.2494, Valid loss: 223.3764\n",
      "Epoch [750/800]: Train loss: 342.6605, Valid loss: 210.9923\n",
      "Epoch [751/800]: Train loss: 344.4077, Valid loss: 220.8384\n",
      "Epoch [752/800]: Train loss: 343.6015, Valid loss: 219.1402\n",
      "Epoch [753/800]: Train loss: 345.2156, Valid loss: 211.8680\n",
      "Epoch [754/800]: Train loss: 348.3895, Valid loss: 215.5913\n",
      "Epoch [755/800]: Train loss: 342.9379, Valid loss: 218.7429\n",
      "Epoch [756/800]: Train loss: 344.6637, Valid loss: 224.8896\n",
      "Epoch [757/800]: Train loss: 349.0620, Valid loss: 220.8104\n",
      "Epoch [758/800]: Train loss: 347.5309, Valid loss: 217.0840\n",
      "Epoch [759/800]: Train loss: 346.7245, Valid loss: 211.0322\n",
      "Epoch [760/800]: Train loss: 345.5170, Valid loss: 225.0079\n",
      "Epoch [761/800]: Train loss: 346.0363, Valid loss: 219.1497\n",
      "Epoch [762/800]: Train loss: 345.9268, Valid loss: 218.4179\n",
      "Epoch [763/800]: Train loss: 347.9031, Valid loss: 217.1770\n",
      "Epoch [764/800]: Train loss: 342.2570, Valid loss: 215.4290\n",
      "Epoch [765/800]: Train loss: 348.8902, Valid loss: 216.6938\n",
      "Epoch [766/800]: Train loss: 345.7376, Valid loss: 217.5935\n",
      "Epoch [767/800]: Train loss: 346.2075, Valid loss: 217.4678\n",
      "Epoch [768/800]: Train loss: 344.7880, Valid loss: 216.9098\n",
      "Epoch [769/800]: Train loss: 345.9620, Valid loss: 220.9475\n",
      "Epoch [770/800]: Train loss: 346.7905, Valid loss: 223.1386\n",
      "Epoch [771/800]: Train loss: 342.1181, Valid loss: 216.1146\n",
      "Epoch [772/800]: Train loss: 347.2489, Valid loss: 213.6897\n",
      "Epoch [773/800]: Train loss: 344.3751, Valid loss: 215.7209\n",
      "Epoch [774/800]: Train loss: 345.8134, Valid loss: 206.5872\n",
      "Epoch [775/800]: Train loss: 346.4418, Valid loss: 214.7932\n",
      "Epoch [776/800]: Train loss: 342.8886, Valid loss: 214.6778\n",
      "Epoch [777/800]: Train loss: 349.5570, Valid loss: 209.2828\n",
      "Epoch [778/800]: Train loss: 344.8003, Valid loss: 221.5882\n",
      "Epoch [779/800]: Train loss: 347.1756, Valid loss: 214.3278\n",
      "Epoch [780/800]: Train loss: 346.3300, Valid loss: 210.0301\n",
      "Epoch [781/800]: Train loss: 350.1391, Valid loss: 219.5574\n",
      "Epoch [782/800]: Train loss: 347.4994, Valid loss: 213.4435\n",
      "Epoch [783/800]: Train loss: 349.5482, Valid loss: 215.3681\n",
      "Epoch [784/800]: Train loss: 344.4910, Valid loss: 210.6993\n",
      "Epoch [785/800]: Train loss: 346.5628, Valid loss: 218.4828\n",
      "Epoch [786/800]: Train loss: 344.8894, Valid loss: 218.6803\n",
      "Epoch [787/800]: Train loss: 342.3091, Valid loss: 216.9244\n",
      "Epoch [788/800]: Train loss: 344.2430, Valid loss: 209.8357\n",
      "Epoch [789/800]: Train loss: 343.7607, Valid loss: 223.0415\n",
      "Epoch [790/800]: Train loss: 347.4289, Valid loss: 210.6556\n",
      "Epoch [791/800]: Train loss: 341.7942, Valid loss: 210.3185\n",
      "Epoch [792/800]: Train loss: 346.0822, Valid loss: 215.8227\n",
      "Epoch [793/800]: Train loss: 345.4459, Valid loss: 220.3283\n",
      "Epoch [794/800]: Train loss: 345.6695, Valid loss: 223.3626\n",
      "Epoch [795/800]: Train loss: 343.2191, Valid loss: 217.8384\n",
      "Epoch [796/800]: Train loss: 346.0168, Valid loss: 214.2417\n",
      "Epoch [797/800]: Train loss: 351.0212, Valid loss: 220.7817\n",
      "Epoch [798/800]: Train loss: 342.9393, Valid loss: 213.1826\n",
      "Epoch [799/800]: Train loss: 343.7573, Valid loss: 207.8726\n",
      "Epoch [800/800]: Train loss: 345.9699, Valid loss: 214.2662\n"
     ]
    }
   ],
   "source": [
    "same_seed(config['seed'])\n",
    "encoder = Encoder(config['input_size'], config['hidden_size'])\n",
    "decoder = Decoder(config['hidden_size'], config['output_size'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_HdR6aeoelZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_HdR6aeoelZ",
    "outputId": "7e36a57e-f8b9-4c98-82c2-affe92356971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 720k/720k [00:03<00:00, 187kB/s]\n",
      "Successfully submitted to 111NTU Homework3"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(config['input_size'], config['hidden_size'])\n",
    "decoder = Decoder(config['hidden_size'], config['output_size'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device, config=config)\n",
    "save_pred(preds.squeeze(), 'b09705017_王紹安.csv')\n",
    "submit_time = (datetime.now() + timedelta(hours=8)).strftime('%m%d%H%M')\n",
    "!kaggle competitions submit -c 111ntu-homework3 -f b09705017_王紹安.csv -m {submit_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUzCbJ0b3Tr6",
   "metadata": {
    "id": "yUzCbJ0b3Tr6"
   },
   "source": [
    "### 手刻 Seq2Seq + attention\n",
    "mse: 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ERCyNu7P3i0z",
   "metadata": {
    "id": "ERCyNu7P3i0z"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 175,      \n",
    "    'select_all': True,   \n",
    "    'valid_ratio': 0.2,  \n",
    "    'n_epochs': 120,        \n",
    "    'batch_size': 512, \n",
    "    'learning_rate': 1e-3,          \n",
    "    'early_stop': 400,    \n",
    "    'save_path': './models/b09705017_王紹安.ckpt',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-5,\n",
    "    'input_size': x_training.shape[2],\n",
    "    'hidden_size': 64,\n",
    "    'output_size': 1,\n",
    "    'teacher_forcing_ratio': 0.5,\n",
    "    'num_layers': 2,\n",
    "    'encoder': {\n",
    "        'dropout': 0.5,\n",
    "    },\n",
    "    'decoder': {\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "PNLqi1xDx3MZ",
   "metadata": {
    "id": "PNLqi1xDx3MZ"
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in).to('cuda')\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        bias = torch.Tensor(size_out).to('cuda')\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x = torch.matmul(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size = 13,\n",
    "                 hidden_size = 64,\n",
    "                 num_layers = 4,\n",
    "                 dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = MyRNN(input_size, hidden_size, num_chunks=num_layers, device='cuda')\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros((x.shape[0], self.hidden_size)).to('cuda')\n",
    "        outputs = torch.zeros((x.shape[1], x.shape[0], self.hidden_size)).to('cuda')\n",
    "        for i, data in enumerate(x.permute(1, 0, 2)):\n",
    "            hidden = self.rnn(data, hidden)\n",
    "            outputs[i, :, :] = hidden\n",
    "        return outputs, hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.rnn = MyRNN(input_size=hidden_size+1, hidden_size=hidden_size, num_chunks=num_layers, device='cuda')\n",
    "        self.fc = MyLinear(hidden_size, 1)\n",
    "\n",
    "        # Attention\n",
    "        self.W1 = MyLinear(hidden_size, hidden_size)\n",
    "        self.W2 = MyLinear(hidden_size, hidden_size)\n",
    "        self.v = MyLinear(hidden_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, decoder_hidden, encoder_output):\n",
    "        # Attention\n",
    "        # (2, 256, 64, 256, 8, 64)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)\n",
    "        decoder_hidden = decoder_hidden.view(decoder_hidden.shape[1], decoder_hidden.shape[0], self.hidden_size) \n",
    "        decoder_hidden = decoder_hidden[:, -1, :].unsqueeze(1) \n",
    "        energy = self.v(self.tanh(self.W1(encoder_output) + self.W2(decoder_hidden)))\n",
    "\n",
    "        attention_weight = self.softmax(energy) \n",
    "        context_vector = torch.sum(attention_weight * encoder_output, dim=1) \n",
    "\n",
    "        concat_input = torch.concat([x, context_vector], dim=-1)\n",
    "        concat_input_fit = concat_input.unsqueeze(1).permute(1, 0, 2) \n",
    "\n",
    "        decoder_hidden = decoder_hidden.permute(1, 0, 2)\n",
    "        for i, (data, hidden) in enumerate(zip(concat_input_fit, decoder_hidden)):\n",
    "            hidden = self.rnn(data, hidden) \n",
    "            output = hidden\n",
    "\n",
    "        output = output.squeeze(1) \n",
    "\n",
    "        prediction = self.fc(output) \n",
    "\n",
    "        return prediction, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        teacher_forcing_ratio: probability of using groud truth instead of training output\n",
    "        set teacher_forcing_ratio to 0 for testing purpose\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        target_len = y.shape[1]\n",
    "        outputs = torch.zeros(y.shape).to(self.device)\n",
    "        encoder_output, hidden = self.encoder(x)\n",
    "        encoder_output = encoder_output.permute(1, 0, 2)\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.float).to(self.device)\n",
    "        for i in range(target_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden, encoder_output)\n",
    "            outputs[:,i] = output\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = y[:,i] if teacher_forcing else output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_chunks: int, device: str):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.weight_ih = nn.Parameter(torch.empty((self.hidden_size, input_size), device=device))\n",
    "        self.weight_hh = nn.Parameter(torch.empty((self.hidden_size, hidden_size), device=device))\n",
    "\n",
    "        self.bias_ih = nn.Parameter(torch.empty(self.hidden_size, device=device))\n",
    "        self.bias_hh = nn.Parameter(torch.empty(self.hidden_size, device=device))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        bound = 1.0 / math.sqrt(hidden_size)\n",
    "\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -bound, bound)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        a = torch.add(torch.mm(x, self.weight_ih.t()), self.bias_ih)\n",
    "        b = torch.add(torch.mm(hidden, self.weight_hh.t()), self.bias_hh)\n",
    "        h = self.tanh(torch.add(a, b))\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "OM40C0i96icn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OM40C0i96icn",
    "outputId": "7792a643-ff1b-430e-a01a-6b0c34a9b706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120]: Train loss: 833.3227, Valid loss: 768.1931\n",
      "Saving model with loss 768.193...\n",
      "Epoch [2/120]: Train loss: 723.1583, Valid loss: 703.9720\n",
      "Saving model with loss 703.972...\n",
      "Epoch [3/120]: Train loss: 657.5162, Valid loss: 650.2147\n",
      "Saving model with loss 650.215...\n",
      "Epoch [4/120]: Train loss: 607.2661, Valid loss: 606.6707\n",
      "Saving model with loss 606.671...\n",
      "Epoch [5/120]: Train loss: 568.1829, Valid loss: 582.6170\n",
      "Saving model with loss 582.617...\n",
      "Epoch [6/120]: Train loss: 532.5788, Valid loss: 558.3920\n",
      "Saving model with loss 558.392...\n",
      "Epoch [7/120]: Train loss: 499.1891, Valid loss: 505.5654\n",
      "Saving model with loss 505.565...\n",
      "Epoch [8/120]: Train loss: 472.5276, Valid loss: 481.7463\n",
      "Saving model with loss 481.746...\n",
      "Epoch [9/120]: Train loss: 447.6411, Valid loss: 454.6034\n",
      "Saving model with loss 454.603...\n",
      "Epoch [10/120]: Train loss: 422.4062, Valid loss: 437.2215\n",
      "Saving model with loss 437.222...\n",
      "Epoch [11/120]: Train loss: 400.0846, Valid loss: 418.0502\n",
      "Saving model with loss 418.050...\n",
      "Epoch [12/120]: Train loss: 382.7738, Valid loss: 399.6187\n",
      "Saving model with loss 399.619...\n",
      "Epoch [13/120]: Train loss: 364.4517, Valid loss: 382.7878\n",
      "Saving model with loss 382.788...\n",
      "Epoch [14/120]: Train loss: 346.8273, Valid loss: 360.6273\n",
      "Saving model with loss 360.627...\n",
      "Epoch [15/120]: Train loss: 334.5513, Valid loss: 340.0368\n",
      "Saving model with loss 340.037...\n",
      "Epoch [16/120]: Train loss: 319.5844, Valid loss: 331.5301\n",
      "Saving model with loss 331.530...\n",
      "Epoch [17/120]: Train loss: 309.6068, Valid loss: 308.5107\n",
      "Saving model with loss 308.511...\n",
      "Epoch [18/120]: Train loss: 298.8154, Valid loss: 308.7890\n",
      "Epoch [19/120]: Train loss: 288.7908, Valid loss: 304.5315\n",
      "Saving model with loss 304.532...\n",
      "Epoch [20/120]: Train loss: 277.5660, Valid loss: 293.3964\n",
      "Saving model with loss 293.396...\n",
      "Epoch [21/120]: Train loss: 268.7490, Valid loss: 279.9784\n",
      "Saving model with loss 279.978...\n",
      "Epoch [22/120]: Train loss: 260.3992, Valid loss: 270.1828\n",
      "Saving model with loss 270.183...\n",
      "Epoch [23/120]: Train loss: 248.9106, Valid loss: 268.8341\n",
      "Saving model with loss 268.834...\n",
      "Epoch [24/120]: Train loss: 240.0808, Valid loss: 259.9470\n",
      "Saving model with loss 259.947...\n",
      "Epoch [25/120]: Train loss: 233.0845, Valid loss: 255.0519\n",
      "Saving model with loss 255.052...\n",
      "Epoch [26/120]: Train loss: 224.3519, Valid loss: 243.5567\n",
      "Saving model with loss 243.557...\n",
      "Epoch [27/120]: Train loss: 215.8958, Valid loss: 234.2103\n",
      "Saving model with loss 234.210...\n",
      "Epoch [28/120]: Train loss: 209.2888, Valid loss: 222.0770\n",
      "Saving model with loss 222.077...\n",
      "Epoch [29/120]: Train loss: 201.6116, Valid loss: 220.4309\n",
      "Saving model with loss 220.431...\n",
      "Epoch [30/120]: Train loss: 198.3470, Valid loss: 215.6997\n",
      "Saving model with loss 215.700...\n",
      "Epoch [31/120]: Train loss: 191.0022, Valid loss: 214.7203\n",
      "Saving model with loss 214.720...\n",
      "Epoch [32/120]: Train loss: 186.0046, Valid loss: 214.4064\n",
      "Saving model with loss 214.406...\n",
      "Epoch [33/120]: Train loss: 178.4296, Valid loss: 195.0688\n",
      "Saving model with loss 195.069...\n",
      "Epoch [34/120]: Train loss: 175.2685, Valid loss: 197.6720\n",
      "Epoch [35/120]: Train loss: 170.2201, Valid loss: 201.2177\n",
      "Epoch [36/120]: Train loss: 167.1486, Valid loss: 185.0818\n",
      "Saving model with loss 185.082...\n",
      "Epoch [37/120]: Train loss: 163.4046, Valid loss: 202.4998\n",
      "Epoch [38/120]: Train loss: 156.9570, Valid loss: 187.7007\n",
      "Epoch [39/120]: Train loss: 154.9665, Valid loss: 181.6038\n",
      "Saving model with loss 181.604...\n",
      "Epoch [40/120]: Train loss: 154.0772, Valid loss: 181.0176\n",
      "Saving model with loss 181.018...\n",
      "Epoch [41/120]: Train loss: 147.5294, Valid loss: 179.5230\n",
      "Saving model with loss 179.523...\n",
      "Epoch [42/120]: Train loss: 146.0053, Valid loss: 169.8637\n",
      "Saving model with loss 169.864...\n",
      "Epoch [43/120]: Train loss: 143.0717, Valid loss: 176.9516\n",
      "Epoch [44/120]: Train loss: 141.1592, Valid loss: 179.0748\n",
      "Epoch [45/120]: Train loss: 135.8163, Valid loss: 162.9356\n",
      "Saving model with loss 162.936...\n",
      "Epoch [46/120]: Train loss: 133.1059, Valid loss: 165.6445\n",
      "Epoch [47/120]: Train loss: 131.6588, Valid loss: 157.1387\n",
      "Saving model with loss 157.139...\n",
      "Epoch [48/120]: Train loss: 128.4518, Valid loss: 153.3391\n",
      "Saving model with loss 153.339...\n",
      "Epoch [49/120]: Train loss: 127.8761, Valid loss: 152.1135\n",
      "Saving model with loss 152.113...\n",
      "Epoch [50/120]: Train loss: 122.9935, Valid loss: 147.0804\n",
      "Saving model with loss 147.080...\n",
      "Epoch [51/120]: Train loss: 119.0942, Valid loss: 150.9400\n",
      "Epoch [52/120]: Train loss: 120.4354, Valid loss: 146.5564\n",
      "Saving model with loss 146.556...\n",
      "Epoch [53/120]: Train loss: 119.9781, Valid loss: 143.3421\n",
      "Saving model with loss 143.342...\n",
      "Epoch [54/120]: Train loss: 118.4714, Valid loss: 141.8453\n",
      "Saving model with loss 141.845...\n",
      "Epoch [55/120]: Train loss: 115.0310, Valid loss: 148.8365\n",
      "Epoch [56/120]: Train loss: 112.9456, Valid loss: 146.7503\n",
      "Epoch [57/120]: Train loss: 110.5853, Valid loss: 148.7206\n",
      "Epoch [58/120]: Train loss: 108.0993, Valid loss: 145.6905\n",
      "Epoch [59/120]: Train loss: 104.8348, Valid loss: 140.0905\n",
      "Saving model with loss 140.090...\n",
      "Epoch [60/120]: Train loss: 104.0336, Valid loss: 143.3954\n",
      "Epoch [61/120]: Train loss: 104.6161, Valid loss: 140.7125\n",
      "Epoch [62/120]: Train loss: 102.9656, Valid loss: 139.8305\n",
      "Saving model with loss 139.830...\n",
      "Epoch [63/120]: Train loss: 103.5786, Valid loss: 159.1348\n",
      "Epoch [64/120]: Train loss: 101.2734, Valid loss: 128.0780\n",
      "Saving model with loss 128.078...\n",
      "Epoch [65/120]: Train loss: 101.8946, Valid loss: 128.6912\n",
      "Epoch [66/120]: Train loss: 99.8526, Valid loss: 126.5525\n",
      "Saving model with loss 126.552...\n",
      "Epoch [67/120]: Train loss: 95.9880, Valid loss: 122.9046\n",
      "Saving model with loss 122.905...\n",
      "Epoch [68/120]: Train loss: 94.7933, Valid loss: 131.6548\n",
      "Epoch [69/120]: Train loss: 92.3214, Valid loss: 131.7483\n",
      "Epoch [70/120]: Train loss: 93.3701, Valid loss: 130.2251\n",
      "Epoch [71/120]: Train loss: 90.8530, Valid loss: 124.2626\n",
      "Epoch [72/120]: Train loss: 90.5649, Valid loss: 125.6088\n",
      "Epoch [73/120]: Train loss: 90.1368, Valid loss: 124.5857\n",
      "Epoch [74/120]: Train loss: 88.7368, Valid loss: 128.8932\n",
      "Epoch [75/120]: Train loss: 88.9661, Valid loss: 128.2494\n",
      "Epoch [76/120]: Train loss: 89.9252, Valid loss: 128.9874\n",
      "Epoch [77/120]: Train loss: 88.8815, Valid loss: 124.0960\n",
      "Epoch [78/120]: Train loss: 86.5108, Valid loss: 128.3126\n",
      "Epoch [79/120]: Train loss: 85.8573, Valid loss: 123.0768\n",
      "Epoch [80/120]: Train loss: 89.7541, Valid loss: 120.4594\n",
      "Saving model with loss 120.459...\n",
      "Epoch [81/120]: Train loss: 90.6770, Valid loss: 123.6902\n",
      "Epoch [82/120]: Train loss: 83.6342, Valid loss: 121.1872\n",
      "Epoch [83/120]: Train loss: 85.0607, Valid loss: 116.0387\n",
      "Saving model with loss 116.039...\n",
      "Epoch [84/120]: Train loss: 85.2588, Valid loss: 123.1893\n",
      "Epoch [85/120]: Train loss: 80.0025, Valid loss: 118.5360\n",
      "Epoch [86/120]: Train loss: 79.5612, Valid loss: 112.8848\n",
      "Saving model with loss 112.885...\n",
      "Epoch [87/120]: Train loss: 79.2717, Valid loss: 115.2075\n",
      "Epoch [88/120]: Train loss: 78.6090, Valid loss: 120.4719\n",
      "Epoch [89/120]: Train loss: 77.7650, Valid loss: 112.2069\n",
      "Saving model with loss 112.207...\n",
      "Epoch [90/120]: Train loss: 78.5117, Valid loss: 130.9604\n",
      "Epoch [91/120]: Train loss: 79.5513, Valid loss: 115.9177\n",
      "Epoch [92/120]: Train loss: 77.6627, Valid loss: 112.9068\n",
      "Epoch [93/120]: Train loss: 76.3893, Valid loss: 115.8072\n",
      "Epoch [94/120]: Train loss: 73.0478, Valid loss: 111.4345\n",
      "Saving model with loss 111.434...\n",
      "Epoch [95/120]: Train loss: 75.9684, Valid loss: 110.1365\n",
      "Saving model with loss 110.136...\n",
      "Epoch [96/120]: Train loss: 72.4572, Valid loss: 113.9586\n",
      "Epoch [97/120]: Train loss: 74.6008, Valid loss: 109.2326\n",
      "Saving model with loss 109.233...\n",
      "Epoch [98/120]: Train loss: 70.4786, Valid loss: 112.1974\n",
      "Epoch [99/120]: Train loss: 73.6243, Valid loss: 109.5174\n",
      "Epoch [100/120]: Train loss: 68.5237, Valid loss: 112.9388\n",
      "Epoch [101/120]: Train loss: 72.6359, Valid loss: 111.6349\n",
      "Epoch [102/120]: Train loss: 75.0054, Valid loss: 118.4240\n",
      "Epoch [103/120]: Train loss: 70.8690, Valid loss: 110.9682\n",
      "Epoch [104/120]: Train loss: 76.6983, Valid loss: 111.5263\n",
      "Epoch [105/120]: Train loss: 75.8101, Valid loss: 114.8467\n",
      "Epoch [106/120]: Train loss: 71.3445, Valid loss: 115.0186\n",
      "Epoch [107/120]: Train loss: 69.4657, Valid loss: 106.6717\n",
      "Saving model with loss 106.672...\n",
      "Epoch [108/120]: Train loss: 67.2375, Valid loss: 109.1776\n",
      "Epoch [109/120]: Train loss: 66.3926, Valid loss: 108.6521\n",
      "Epoch [110/120]: Train loss: 68.8036, Valid loss: 110.1563\n",
      "Epoch [111/120]: Train loss: 67.6701, Valid loss: 107.5140\n",
      "Epoch [112/120]: Train loss: 67.6454, Valid loss: 107.0924\n",
      "Epoch [113/120]: Train loss: 68.4815, Valid loss: 102.6722\n",
      "Saving model with loss 102.672...\n",
      "Epoch [114/120]: Train loss: 68.9385, Valid loss: 101.8150\n",
      "Saving model with loss 101.815...\n",
      "Epoch [115/120]: Train loss: 64.8949, Valid loss: 102.6285\n",
      "Epoch [116/120]: Train loss: 65.7445, Valid loss: 102.3350\n",
      "Epoch [117/120]: Train loss: 67.0947, Valid loss: 104.3338\n",
      "Epoch [118/120]: Train loss: 66.0171, Valid loss: 108.0553\n",
      "Epoch [119/120]: Train loss: 67.2953, Valid loss: 102.7659\n",
      "Epoch [120/120]: Train loss: 63.5869, Valid loss: 103.3337\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size=config['input_size'], hidden_size=config['hidden_size'], num_layers=config['num_layers'], dropout=config['encoder']['dropout'])\n",
    "decoder = Decoder(config['hidden_size'], num_layers=config['num_layers'], dropout=config['decoder']['dropout'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "trainer(train_loader, valid_loader, model, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5FwUn39nAmQG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FwUn39nAmQG",
    "outputId": "8dca9eee-9382-401e-c166-6226d37a1cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 715k/715k [00:01<00:00, 612kB/s]\n",
      "Successfully submitted to 111NTU Homework3"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size=config['input_size'], hidden_size=config['hidden_size'], num_layers=config['num_layers'], dropout=config['encoder']['dropout'])\n",
    "decoder = Decoder(config['hidden_size'], num_layers=config['num_layers'], dropout=config['decoder']['dropout'])\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader, model, device, config=config)\n",
    "save_pred(preds.squeeze(), 'b09705017_王紹安.csv')\n",
    "submit_time = (datetime.now() + timedelta(hours=8)).strftime('%m%d%H%M')\n",
    "!kaggle competitions submit -c 111ntu-homework3 -f b09705017_王紹安.csv -m {submit_time}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.032084,
   "end_time": "2022-09-19T05:17:25.716651",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-19T05:17:15.684567",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
